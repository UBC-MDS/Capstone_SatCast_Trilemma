Despite these gains, several constraints remain. The dataset used spans just two months, limiting exposure to rare but impactful phenomena. Many features that help explain fee behavior—such as mempool congestion or block composition—are available only with a lag, reducing our ability to predict true leading signals. Furthermore, while deep models like TFT are powerful, they require regular retraining and high computational resources, which may constrain deployment or accessibility. Our custom loss function, though aligned with business needs, remains static and could be better tuned to balance volatility sensitivity and accuracy.

Looking forward, future work could explore alternate data sources and modeling strategies that address these constraints. First, longer historical coverage or the inclusion of off-chain indicators may improve predictive range. Second, deeper exploration and adaptive tuning of the custom loss function may further align model behavior with user pain points around volatility and timing. Third, future approaches might benefit from hybrid pipelines—where intermediate signals (e.g., mempool congestion or block inclusion rates) are predicted first and then fed into the final fee forecasting model. Finally, probabilistic forecasting and uncertainty quantification could add value once magnitude prediction improves. 