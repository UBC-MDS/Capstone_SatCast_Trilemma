## Evaluation Metrics

We used multiple metrics to evaluate the forecasting models, selected based on both standard practice and alignment with stakeholder needs. The Mean Absolute Percentage Error (MAPE) served as the primary metric due to its interpretability and widespread use in forecasting tasks. In addition, we used Root Mean Squared Error (RMSE) to assess each model's ability to capture spikes, since RMSE penalizes large errors more heavily—an important consideration for volatile fee behavior.

To better capture the characteristics of Bitcoin fee spikes, we designed a custom composite loss function for model training. This loss combines three components: base error (MAE), volatility mismatch (standard deviation loss), and spike timing error (deviation error), which is calculated as: Total Loss = Base Loss + Std Loss + Deviation Error. While traditional losses like MAE or MSE tend to smooth over volatility, our formulation explicitly rewards models that preserve sharp changes and dynamic patterns. A breakdown of each term is shown in Table @tbl-custom-loss.

::: {.table #tbl-custom-loss}

Table: Breakdown of custom loss function components.

| **Component**     | **Base Loss**            | **Std Loss**                | **Deviation Error**                                  |
|-------------------|--------------------------|-----------------------------|------------------------------------------------------|
| **Calculation**   | `y_pred − y_true`        | `std_pred − std_true`       | `(y_pred − ȳ_pred) − (y_true − ȳ_true)`             |
| **Captures**   | Raw error                | Overall volatility mismatch | Dynamic (pointwise) pattern mismatch                 |
| **Relevance to Spikes** | Underweights spikes | Penalizes smoothing         | Captures spike timing                                |
:::

This design was inspired by volatility-sensitive training regimes used in interpretable deep time series models such as the Temporal Fusion Transformer [@lim2019temporal].

## Stakeholder Impact & Potential Ethical Concerns / Limitations

Our evaluation metrics and modeling choices were designed to align with stakeholder priorities. For end users, forecasting the timing of high-fee periods is more valuable than precise predictions, as it enables them to avoid costly transaction windows. Wallet developers use these forecasts to provide timely fee suggestions, while miners may use them to optimize revenue.

However, deploying such tools raises ethical concerns. Users may over-rely on forecasts, mistaking them for guarantees. To address this, outputs should communicate uncertainty and limitations. There is also a risk of manipulation—public forecasts could be exploited to game mempool behavior. In addition, unequal access to forecasting tools may reinforce disparities. Promoting transparency and open access helps mitigate these risks.

Some broader concerns remain unaddressed, including fairness across user groups, potential shifts in miner incentives, and susceptibility to coordinated manipulation. Future work should consider these dimensions through equity-aware metrics and policy-informed design.
