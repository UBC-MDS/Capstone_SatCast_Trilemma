## Models   

To understand and anticipate Bitcoin transaction fee rate dynamics, we implemented a progressive series of models. Each model was chosen to overcome specific limitations identified in its predecessor, forming an iterative pipeline that is summarized in @fig-models. Below, we describe each model, its contributions, and where it ultimately fell short.

![Model progression from simple baselines to deep learning, each addressing prior gaps.](../../img/model_progression.png){#fig-models width=100%}
```{python}
import pandas as pd
df = pd.read_parquet('../../data/raw/mar_5_may_12.parquet')
fee_cols = [col for col in df.columns if col.startswith("mempool_fee_histogram_bin_")]
total = df[fee_cols].values.sum()
num1_2 = df["mempool_fee_histogram_bin_1_2"].values.sum()
num2_3 = df["mempool_fee_histogram_bin_2_3"].values.sum()

```
### Dummy Model (Global Median)

We began with a dummy model that predicted a constant value: the global median of the fee rate series. As shown in the distribution of fastest fee rates (@fig-fastestfee-distribution), roughly `{python} f"{(num1_2/total*100):.1f}"`% of all transactions fell into the 1–2 sats/vB bin; including 2–3 sats/vB brought the cumulative share above `{python} f"{((num1_2+num2_3)/total*100):.1f}"`%. It made the median a surprisingly strong benchmark. While this model ignored all temporal and contextual variation, it provided a reference point to quantify the inherent difficulty of the forecasting task. No training or hyperparameter tuning was required for this static approach.

### Holt-Winters Exponential Smoothing (HWES)

We then implemented Holt-Winters Exponential Smoothing given clear seasonality and trend observed in our decomposition analysis (@fig-decomp). This method supports both additive and multiplicative structures, as well as optional damped trends. We performed a grid search to identify the optimal configuration of these components. However, residual autocorrelation observed in ACF and PACF plots (@fig-acf) indicated that HWES failed to capture deeper temporal dependencies, limiting its forecasting accuracy.

![FastestFee exhibits multiplicative trend and seasonality structure.](../../results/plots/decomposition_multiplicative.png){#fig-decomp width=100%}

### SARIMA

To address the temporal dependencies missed by HWES, we introduced SARIMA that combines autoregressive, differencing, and moving average components to model time-dependent and seasonal patterns. Drawing on exploratory analysis, including strong autocorrelation at short lags, partial autocorrelation structure, and daily seasonality, we manually selected parameters to reflect short-term dynamics and recurring cycles in the fee series. However, SARIMA does not support exogenous variables, which limited our ability to incorporate external factors such as transaction volume or mempool congestion.

![Residual autocorrelation highlights unmodeled temporal dependencies.](../../results/plots/acf_pacf_plot.png){#fig-acf width=100%}

### XGBoost
```{python}
from scipy.stats import spearmanr
# Selected features for correlation analysis
corr_features = [
    "recommended_fee_fastestFee",
    "mempool_total_fee",
    "mempool_count",
    "mempool_blocks_nTx",
    "mempool_blocks_blockVSize",
    "difficulty_adjustment_difficultyChange",
    "mempool_blocks_totalFees"
]
# Map for nicer axis labels
label_map = {
    "recommended_fee_fastestFee": "Fastest Fee",
    "mempool_total_fee": "Mempool Total Fee",
    "mempool_count": "Mempool Count",
    "mempool_blocks_nTx": "Next Block # of Transaction",
    "mempool_blocks_blockVSize": "Block vSize",
    "difficulty_adjustment_difficultyChange": "Difficulty Change",
    "mempool_blocks_totalFees": "Next Block Total Fee"
}

# Initialize empty DataFrames for correlation and p-values
spearman_corr = pd.DataFrame(index=corr_features, columns=corr_features)

# Compute Spearman correlation for each pair
for i in corr_features:
    for j in corr_features:
        x = df[i].dropna()
        y = df[j].dropna()
        common_idx = x.index.intersection(y.index)
        if len(common_idx) > 2:
            r_s, p_s = spearmanr(x[common_idx], y[common_idx])
            spearman_corr.loc[i, j] = r_s

# Rename labels for plot
spearman_corr = spearman_corr.astype(float).rename(index=label_map, columns=label_map)

```
We next adopted XGBoost to leverage a broader set of concurrent features. As shown in the correlation heatmap (@fig-spearman-heat), variables such as mempool total fee(`{python} f"""{(round(spearman_corr.at["Fastest Fee", "Mempool Total Fee"],2)):.2f}"""`), mempool count(`{python} f"""{(round(spearman_corr.at["Fastest Fee", "Mempool Count"],2)):.2f}"""`), and projected block fees(`{python} f"""{(round(spearman_corr.at["Fastest Fee", "Next Block Total Fee"],2)):.2f}"""`) were strongly correlated with the fastest fee rate. XGBoost, a gradient-boosted tree model, enabled us to capture non-linear interactions among these features. It expanded our modeling capacity beyond univariate structures and allowed fine-grained feature importance analysis. We tuned the model using randomized search over tree depth, learning rate, and regularization terms.

![Strong correlations observed between target fee rate and key explanatory variables.](../../results/plots/spearman_correlation.png){#fig-spearman-heat width=100%}

### Prophet

Then we moved to advanced models. First we extended the HWES baseline with Prophet to better handle abrupt changes in fee trends. While both methods model trend and seasonality, Prophet improves flexibility by incorporating automatic changepoint detection and user-defined seasonal effects. We configured Prophet with hourly, daily, and weekly seasonalities to reflect recurring fee cycles, and tuned changepoint sensitivity to capture sudden demand shifts such as batch fee spikes or market congestion.

### DeepAR

Building on the autoregressive structure of SARIMA and the feature integration strength of XGBoost, we explored DeepAR—an LSTM-based model developed by Amazon for probabilistic sequence forecasting. Unlike traditional models, DeepAR learns global patterns across sequences and captures non-linear temporal dependencies, while also allowing the inclusion of time-varying and static covariates. We used PyTorch Lightning’s Trainer to manage training configuration and Tuner to automatically search for optimal hyperparameters, including learning rate and hidden units. Early stopping was applied to prevent overfitting and stabilize convergence. This setup allowed us to model sequential fee dynamics more expressively and robustly.

### Temporal Fusion Transformer (TFT)

Finally, we introduced the Temporal Fusion Transformer (TFT), the most advanced model in our pipeline. TFT builds on DeepAR’s ability to incorporate time-varying covariates and static metadata, while advancing the architecture with explicit variable selection, multi-horizon attention, and interpretable output mechanisms. This allows TFT to capture richer dependencies and highlight feature importance in ways DeepAR cannot. We carefully optimized the TFT architecture, including variable selection layers, gated residual networks, and learning rate schedules. This model allowed us to capture complex interactions and feature importances while generating multi-horizon probabilistic forecasts.

### Considered Alternatives and Limitations

Our efforts extended beyond conventional forecasting once it became clear that traditional models failed to capture spike behavior. We reframed the problem as a classification task to predict spike occurrences within a time window, but the rarity and unpredictability of these events made the approach unreliable. We also explored methods like Distributed Lag Non-Linear Models (DLNM) and Fourier transforms, but their assumptions did not fit the data, as fee spikes lacked consistent leading signals and showed no periodic structure.

A major limitation in our modeling was the reliance on lagged exogenous features, which constrained the model’s ability to anticipate sudden fee spikes. To address this, we considered a two-stage forecasting setup in which key indicators would be predicted first and then fed into the fee model to enable more forward-looking forecasts. However, given the lack of reliable leading signals, the partner’s emphasis on fully utilizing existing features, and the risk of compounding errors in multi-stage pipelines, we chose to prioritize model depth over input expansion within the project’s limited timeframe.