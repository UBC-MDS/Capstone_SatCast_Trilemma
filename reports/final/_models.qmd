## Models

To understand and anticipate Bitcoin transaction fee rate dynamics, we implemented a sequence of models. Each of the models were selected for its ability to address specific limitations observed in the previous ones. Below, we walk through these choices, results, and where each model fell short.

### Dummy Model (Global Median)

We began with a simple dummy model. From @fig-fastestfee-distribution, it could be illustrated that over 92.5% of observed fee rates fell within a narrow band of 2–3 satoshis/byte, so the median itself was already a strong first guess. Thus, our dummy model always predicted the global median transaction fee rate.  Although it had no predictive power, it served as a baseline to measure improvements from more sophisticated approaches. This model completely ignored any temporal or external structure in the data, but offered a useful starting point to quantify how difficult the prediction task really was.

### Holt-Winters Exponential Smoothing (HWES)

Given clear seasonality observed in our decomposition analysis @fig-decomp, Holt-Winters Exponential Smoothing was a natural next step. It captured seasonality fairly well and improved upon the dummy model. However, analysis of the residuals in @fig-acf revealed persistent autocorrelation, suggesting that the model failed to account for important lag effects or hidden patterns beyond periodic behavior.

![Multiplicative decomposition of recommended fastest fee rate](../../results/plots/decomposition_multiplicative.png){#fig-decomp width=100%}

### SARIMA

To address the temporal dependencies missed by HWES, we adopted SARIMA, which supported autoregressive and seasonal differencing components. Because of its capacity to learn from prior values, SARIMA produced a better short-term fit. However, its univariate nature prevented us from including important exogenous features like transaction count, mempool congestion, or size distributions. That limited its practical usefulness in a multi-variable environment.

![ACF and PACF](../../results/plots/acf_pacf_plot.png){#fig-acf width=100%}

### XGBoost

To move beyond univariate models, we adopted XGBoost, which allowed us to leverage the full multivariate feature set. Correlation analysis in @fig-spearman-heat showed that recommended fastest fee rate was significantly influenced by several concurrent indicators. XGBoost is a powerful tree-based model capable of incorporating a broad array of features. It significantly expanded our input space and enabled non-linear interactions among variables. While its numerical performance improved, especially on average error metrics like MAPE, the model still struggled with volatility. It produced smooth and flat outputs that failed to capture sudden fee spikes. However, those spikes were precisely the events most critical for users.

![Spearman correlation heatmap](../../results/plots/spearman_correlation.png){#fig-spearman-heat width=100%}

### Prophet

We explored Facebook's Prophet model to take advantage of its flexibility in modeling seasonality, changepoints, and custom events. Prophet brought in useful priors for time series with irregular behavior and offered a simple interface for integrating domain knowledge. Unfortunately, it still smoothed over the spikes and underperformed in capturing real-time fee volatility. Its strength in trend estimation did not translate well to our highly reactive use case.

### DeepAR

To try more dynamic modeling, we implemented DeepAR, which was an LSTM-based autoregressive forecasting model. In theory, DeepAR should have leveraged temporal context more effectively and handled sequential data better. However, the outputs were unstable and noisy. The results often failed to align with real-world fee movements. The model demonstrated limited generalizability, and its probabilistic forecasts often appeared more random than informative.


### Temporal Fusion Transformer (TFT)

Our final and most advanced model was the Temporal Fusion Transformer(TFT). TFT was designed to integrate static covariates, time-varying features, attention mechanisms, and variable selection into a unified deep learning architecture. Among all models, TFT came closest to capturing both overall volatility and individual spike events. It successfully learned temporal dependencies, responded to feature relevance dynamically, and produced the most realistic forecasts. While computationally expensive and complex to tune, its performance and interpretability made it the strongest candidate for this forecasting task.


### Considered Alternatives and Limitations

A major limitation of our approach is the reliance on lagged exogenous features (e.g., mempool stats), which limits the model’s ability to anticipate fee spikes. One potential improvement is a two-stage setup: first, forecast future values of key features; then, feed those predictions into the fee model for better forward-looking performance.

We did not pursue this path due to both practical and strategic reasons. The partner emphasized focusing on existing features before adding external signals and noted prior attempts at using sentiment data (e.g., scraped news, tweets) yielded poor results. Multi-stage forecasting also risks compounding errors, and with limited time, we prioritized evaluating diverse model architectures over expanding feature pipelines.
