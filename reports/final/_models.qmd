## Models

To understand and anticipate Bitcoin transaction fee rate dynamics, we implemented a sequence of models. Each of the models were selected for its ability to address specific limitations observed in the previous ones. Below, we walk through these choices, results, and where each model fell short.

### Dummy Model (Global Median)

We began with a simple dummy model. From @fig-fastest-dist, it could be illustrated that over 92.5% of observed fee rates fell within a narrow band of 2â€“3 satoshis/byte, so the median itself was already a strong first guess. Thus, our dummy model always predicted the global median transaction fee rate.  Although it had no predictive power, it served as a baseline to measure improvements from more sophisticated approaches. This model completely ignored any temporal or external structure in the data, but offered a useful starting point to quantify how difficult the prediction task really was.


![Distribution of recommended fastest fee rates](../../img/fastest_fee_distribution.png){#fig-fastest-dist width=100%}

### Holt-Winters Exponential Smoothing (HWES)

Given clear seasonality observed in our decomposition analysis @fig-decomp, Holt-Winters Exponential Smoothing was a natural next step. It captured seasonality fairly well and improved upon the dummy model. However, analysis of the residuals in @fig-acf revealed persistent autocorrelation, suggesting that the model failed to account for important lag effects or hidden patterns beyond periodic behavior.

![Multiplicative decomposition of recommended fastest fee rate](../../img/decomposition_multiplicative.png){#fig-decomp width=100%}

### SARIMA

To address the temporal dependencies missed by HWES, we adopted SARIMA, which supported autoregressive and seasonal differencing components. Because of its capacity to learn from prior values, SARIMA produced a better short-term fit. However, its univariate nature prevented us from including important exogenous features like transaction count, mempool congestion, or size distributions. That limited its practical usefulness in a multi-variable environment.

![ACF and PACF](../../img/acf_pacf_plot.png){#fig-acf width=100%}

### XGBoost

To move beyond univariate models, we adopted XGBoost, which allowed us to leverage the full multivariate feature set. Correlation analysis in @fig-pearson-heat and @fig-spearman-heat showed that recommended fastest fee rate was significantly influenced by several concurrent indicators. XGBoost is a powerful tree-based model capable of incorporating a broad array of features. It significantly expanded our input space and enabled non-linear interactions among variables. While its numerical performance improved, especially on average error metrics like MAPE, the model still struggled with volatility. It produced smooth and flat outputs that failed to capture sudden fee spikes. However, those spikes were precisely the events most critical for users.

![Pearson correlation heatmap](../../img/pearson_correlation.png){#fig-pearson-heat width=100%}

![Spearman correlation heatmap](../../img/spearman_correlation.png){#fig-spearman-heat width=100%}

### Prophet

We explored Facebook's Prophet model to take advantage of its flexibility in modeling seasonality, changepoints, and custom events. Prophet brought in useful priors for time series with irregular behavior and offered a simple interface for integrating domain knowledge. Unfortunately, it still smoothed over the spikes and underperformed in capturing real-time fee volatility. Its strength in trend estimation did not translate well to our highly reactive use case.

### DeepAR

To try more dynamic modeling, we implemented DeepAR, which was an LSTM-based autoregressive forecasting model. In theory, DeepAR should have leveraged temporal context more effectively and handled sequential data better. However, the outputs were unstable and noisy. The results often failed to align with real-world fee movements. The model demonstrated limited generalizability, and its probabilistic forecasts often appeared more random than informative.


### Temporal Fusion Transformer (TFT)

Our final and most advanced model was the Temporal Fusion Transformer(TFT). TFT was designed to integrate static covariates, time-varying features, attention mechanisms, and variable selection into a unified deep learning architecture. Among all models, TFT came closest to capturing both overall volatility and individual spike events. It successfully learned temporal dependencies, responded to feature relevance dynamically, and produced the most realistic forecasts. While computationally expensive and complex to tune, its performance and interpretability made it the strongest candidate for this forecasting task.


### What Might Have Worked Better

#### Current Limitation
One key limitation was that most exogenous features available to the model were lagged. As a result, the models could only react to changes in mempool congestion, transaction volume, or block size once they had already occurred. This lag reduced the models' ability to anticipate future spikes. Furthermore, our dataset lacked strong leading indicators. Without predictive signals of future fee pressure, the models had no way of forecasting sudden changes in transaction costs. Compounding this challenge was the inherent nature of spikes themselves. These events were volatile, abrupt, and highly irregular, making them difficult to learn from historical data and nearly impossible to generalize across different time periods.

#### Potential Improvement
To address these issues, a more forward-looking architecture could potentially improve performance. One such direction is multi-stage forecasting. In this setup, we would first build separate models to predict future values of critical exogenous variables, such as mempool congestion or the number of unconfirmed transactions. The outputs of these auxiliary models could then serve as inputs into the primary model responsible for predicting fee rates. This approach would transform the problem from a reactive forecast to a more anticipatory one.

#### Why Not Implemented
However, this methodology was not implemented for several reasons. During the check-in with our partner, it was made clear that they preferred us to fully explore the performance of the existing features before expanding into predicted or synthetic indicators. Moreover, the accuracy of these predicted exogenous signals would be uncertain, and there was a risk of introducing compounding forecast errors across the two stages. Our partner also shared that they had previously attempted to use social sentiment analysis based on scraped news and tweets, but the signals generated were too noisy and ultimately unhelpful. Given these constraints and the limited time available, the team decided to focus on maximizing what could be learned from the current dataset through careful model selection and tuning.
