## Models   

As part of our modeling strategy, we implemented a series of progressively refined models. Each was selected to address limitations of its predecessor, forming an iterative pipeline summarized in @fig-models. Below, we describe each model, outline its contributions and remaining challenges.

![Model progression from simple baselines to deep learning, each addressing prior gaps.](../../img/model_progression.png){#fig-models width=100%}

```{python}
import pandas as pd
df = pd.read_parquet('../../data/raw/mar_5_may_12.parquet')
fee_cols = [col for col in df.columns if col.startswith("mempool_fee_histogram_bin_")]
total = df[fee_cols].values.sum()
num1_2 = df["mempool_fee_histogram_bin_1_2"].values.sum()
num2_3 = df["mempool_fee_histogram_bin_2_3"].values.sum()

```

### Dummy Model (Global Median)

We began with a dummy model that predicted a constant value: the global median of the fee rate series. As shown in the distribution of fastest fee rates (@fig-fastestfee-distribution), approximately `{python} f"{(num1_2/total*100):.1f}"`% of all transactions fell into the 1–2 sats/vB bin, and including 2–3 sats/vB raised the cumulative share above `{python} f"{((num1_2+num2_3)/total*100):.1f}"`%. This made the median a surprisingly strong benchmark. Although the model ignored all temporal and contextual information, it served as a useful reference for evaluating task difficulty. No training or tuning was required.

### Holt-Winters Exponential Smoothing (HWES)

We then implemented HWES to model the clear trend and seasonality revealed by our decomposition analysis (@fig-decomp). HWES supports both additive and multiplicative structures, along with optional damped trends. We used grid search to optimize these components.

![FastestFee exhibits multiplicative trend and seasonality structure.](../../results/plots/decomposition_multiplicative.png){#fig-decomp width=100%}

However, autocorrelation patterns observed in ACF and PACF plots (@fig-acf) indicated that HWES failed to capture deeper temporal dependencies, limiting its forecasting accuracy.

![Residual autocorrelation highlights unmodeled temporal dependencies.](../../results/plots/acf_pacf_plot.png){#fig-acf width=100%}

### SARIMA

To address the temporal dependencies missed by HWES, we introduced SARIMA that combines autoregressive, differencing, and moving average components to model time-dependent and seasonal patterns. Based on exploratory analysis—strong short-lag autocorrelation, partial autocorrelation structure, and daily seasonality—we manually selected parameters to reflect short-term dynamics and recurring cycles. However, SARIMA lacks support for exogenous variables, limiting our ability to incorporate other signals such as transaction volume or mempool congestion.

### XGBoost
```{python}
from scipy.stats import spearmanr
# Selected features for correlation analysis
corr_features = [
    "recommended_fee_fastestFee",
    "mempool_total_fee",
    "mempool_count",
    "mempool_blocks_nTx",
    "mempool_blocks_blockVSize",
    "difficulty_adjustment_difficultyChange",
    "mempool_blocks_totalFees"
]
# Map for nicer axis labels
label_map = {
    "recommended_fee_fastestFee": "Fastest Fee",
    "mempool_total_fee": "Mempool Total Fee",
    "mempool_count": "Mempool Count",
    "mempool_blocks_nTx": "Next Block # of Transaction",
    "mempool_blocks_blockVSize": "Block vSize",
    "difficulty_adjustment_difficultyChange": "Difficulty Change",
    "mempool_blocks_totalFees": "Next Block Total Fee"
}

# Initialize empty DataFrames for correlation and p-values
spearman_corr = pd.DataFrame(index=corr_features, columns=corr_features)

# Compute Spearman correlation for each pair
for i in corr_features:
    for j in corr_features:
        x = df[i].dropna()
        y = df[j].dropna()
        common_idx = x.index.intersection(y.index)
        if len(common_idx) > 2:
            r_s, p_s = spearmanr(x[common_idx], y[common_idx])
            spearman_corr.loc[i, j] = r_s

# Rename labels for plot
spearman_corr = spearman_corr.astype(float).rename(index=label_map, columns=label_map)

```

We next adopted XGBoost to leverage a broader set of concurrent features. As shown in the correlation heatmap (@fig-spearman-heat), variables such as *mempool total fee*(`{python} f"""{(round(spearman_corr.at["Fastest Fee", "Mempool Total Fee"],2)):.2f}"""`), *mempool count*(`{python} f"""{(round(spearman_corr.at["Fastest Fee", "Mempool Count"],2)):.2f}"""`), and *projected block fees*(`{python} f"""{(round(spearman_corr.at["Fastest Fee", "Next Block Total Fee"],2)):.2f}"""`) were strongly correlated with the *fastestFee*. XGBoost, a gradient-boosted tree model, captured non-linear interactions among features and extended our modeling beyond univariate structures. It also enabled detailed feature importance analysis. We tuned it using randomized search over tree depth, learning rate, and regularization parameters.

![Strong correlations observed between target fee rate and key explanatory variables.](../../results/plots/spearman_correlation.png){#fig-spearman-heat width=100%}

### Prophet

Then we moved to advanced models. First we extended the HWES baseline using Prophet to better handle abrupt changes in fee trends. Like HWES, Prophet models trend and seasonality, but adds flexibility through automatic changepoint detection and customizable seasonal effects. We configured hourly, daily, and weekly seasonalities to reflect recurring fee cycles and tuned changepoint sensitivity to capture sudden shifts, such as batch fee spikes or market congestion.

### DeepAR

Building on SARIMA’s autoregressive structure and XGBoost’s feature integration, we explored DeepAR—an LSTM-based model designed for probabilistic sequence forecasting. DeepAR captures non-linear temporal dependencies and learns global patterns across sequences, while supporting both time-varying and static covariates. We used PyTorch Lightning’s *Trainer* for training management and *Tuner* for hyperparameter optimization, including learning rate and hidden units. Early stopping was applied to prevent overfitting and ensure stable convergence. This setup enabled more expressive and robust modeling of sequential fee dynamics.

### Temporal Fusion Transformer (TFT)

Finally, we introduced TFT, the most advanced model in our pipeline. TFT extends DeepAR by incorporating explicit variable selection, multi-horizon attention, and interpretable outputs, while preserving support for time-varying covariates and static metadata. These enhancements allow it to capture richer dependencies and reveal feature importance more effectively. We carefully optimized the architecture by tuning variable selection layers, gated residual networks, and learning rate schedules. TFT enabled us to model complex interactions and produce multi-horizon probabilistic forecasts with greater interpretability.


### Considered Alternatives and Limitations

Our efforts extended beyond conventional forecasting once it became clear that traditional models failed to capture spike behavior. We reframed the problem as a classification task to predict spike occurrences within a time window, but the rarity and unpredictability of these events made the approach unreliable. We also explored methods like Distributed Lag Non-Linear Models (DLNM) and Fourier transforms, but their assumptions did not fit the data, as fee spikes lacked consistent leading signals and showed no periodic structure.

A major limitation in our modeling was the reliance on lagged exogenous features, which constrained the model’s ability to anticipate sudden fee spikes. To address this, we considered a two-stage pipeline: first forecasting key indicators, then feeding them into the fee model. However, due to the absence of reliable leading signals, the partner’s emphasis on fully leveraging existing features, and the risk of compounding errors in multi-stage setups, we prioritized model depth over input expansion within the project’s limited timeframe.