## Models

To understand and anticipate Bitcoin transaction fee rate dynamics, we implemented a sequence of models. Each of the models were selected for its ability to address specific limitations observed in the previous ones. Below, we walk through these choices, results, and where each model fell short.

### Dummy Model (Global Median)

We began with a simple dummy model that always predicted the global median fee rate. It could be observed in the distribution of fastest fee rates (@fig-fastestfee-distribution) that over 92.5% of observed fee rates fell within a narrow band of 2–3 satoshis/byte. Thus, the median itself was already a strong baseline.  Although it ignored all temporal or external patterns, we could still quantify how difficult the prediction task really was. No hyperparameter tuning was needed for this static model.

### Holt-Winters Exponential Smoothing (HWES)

Given clear seasonality observed in our decomposition analysis (@fig-decomp), Holt-Winters Exponential Smoothing was a natural next step. This time series method accounted for both trend and periodic fluctuations. We used grid search to determine trend components, seasonal components and trend options. However, autocorrelation analysis of residuals (@fig-acf) showed persistant autocorrelation. The result suggested that HWES did not fully capture temporal dependencies. 

![Multiplicative decomposition of recommended fastest fee rate](../../results/plots/decomposition_multiplicative.png){#fig-decomp width=100%}

### SARIMA

To address the temporal dependencies missed by HWES, we introduced SARIMA to better handle autocorrelation and lagged temporal structures. SARIMA used autoregressive, differencing and moving average components to model time-dependent patterns. Based on the analysis of EDA including strong autocorrelation at short lags, partial autocorrelation and daily seasonality, we select appropriate hyperparameters manually to capture both short-term dynamics and daily seasonal cycles in the fee series. However, this model prevented us from using exogenous features such as transaction counts or mempool congestion.

![ACF and PACF](../../results/plots/acf_pacf_plot.png){#fig-acf width=100%}

### XGBoost

To incorporate a richer feature set, we adopted XGBoost. The correlation heatmap (@fig-spearman-heat) showed strong correlations between recommended fastest fee rate and several concurrent indicators like mempool total fee, mempool count or next block total fee. XGBoost is a powerful tree-based model capable of incorporating a broad array of features. It significantly expanded our input space and enabled non-linear interactions among variables. We performed hyperparameter tuning using randomized search over tree depth, learning rate and regularization terms. 

![Spearman correlation heatmap](../../results/plots/spearman_correlation.png){#fig-spearman-heat width=100%}

### Prophet

Then we moved to advanced models. The first one we tried was Meta's Prophet model. It had the ability to model trend shifts, seasonality, changepoints and custom events. Prophet brought in useful priors for time series with irregular behavior and offered a simple interface for integrating domain knowledge. We configured custom seasonal components including hourly, daily and weekly ones and tuned changepoint and seasonality parameters to adapt to the reactive nature of fee dynamics. 

### DeepAR

To try more dynamic modeling, we implemented DeepAR. It was an LSTM-based autoregressive forecasting model. In theory, DeepAR should have leveraged temporal context more effectively and handled sequential data better. To optimize the model, we used PyTorch Lightning’s "Trainer" to manage training configuration, ran "Tuner " to automatically find optimal hyperparameters and applied early stop to reduce overfitting. 


### Temporal Fusion Transformer (TFT)

Our final and most advanced model was the Temporal Fusion Transformer(TFT). TFT was designed to integrate static covariates, time-varying features, attention mechanisms, and variable selection into a unified deep learning architecture. We carefully configured the final TFT model architecture, training callbacks, optimization settings and learning rate scheduler to ensure stable training, efficient convergence and robust generalization.


### Considered Alternatives and Limitations

A major limitation of our approach is the reliance on lagged exogenous features. They limits the model’s ability to anticipate fee spikes. One potential improvement is a two-stage setup: first, forecast future values of key features; then, feed those predictions into the fee model for better forward-looking performance.

We did not pursue this path due to both practical and strategic reasons. The partner emphasized focusing on existing features before adding external signals and noted prior attempts at using sentiment data yielded poor results. Multi-stage forecasting also risks compounding errors. Thus, with limited time, we prioritized evaluating diverse model architectures over expanding feature pipelines.
