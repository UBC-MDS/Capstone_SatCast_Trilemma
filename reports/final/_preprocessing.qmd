## Feature Preprocessing

We conducted exploratory correlation analysis to better understand the data. This analysis confirmed that several predictors carry meaningful signals related to the target variable fastestFee. This was not used for formal feature selection, but helped validate feature relevance.

```{python}
import sys
from pathlib import Path
from scipy.stats import pearsonr

project_root = Path().resolve().parent.parent
src_path = project_root / "src"
sys.path.append(str(src_path))
from preprocess_raw_parquet import preprocess_raw_parquet

df = preprocess_raw_parquet('../../data/raw/mar_5_may_12.parquet')
x = df["recommended_fee_halfHourFee"].dropna()
y = df["recommended_fee_fastestFee"].dropna()
common_idx = x.index.intersection(y.index)
r, _ = pearsonr(x.loc[common_idx], y.loc[common_idx])
```

To avoid target leakage, we excluded features with extremely high correlation to the target. For instance, halfHourFee showed a correlation of `{python} f"{r:.2f}"` with fastestFee. These were excluded from all models except DeepAR and TFT, where model constraints made removal more complex.

We also applied several feature engineering steps. We first examined the distribution of fastestFee and found it to be highly right-skewed, with most values clustered at the low end and a few extreme spikes shown in @fig-fastestfee-distribution. This kind of heavy skew can adversely affect model stability and violate assumptions required by some forecasting models.

```{python}
#| label: fig-fastestfee-distribution
#| fig-cap: "The distribution of fastestFee shows strong right skew."
#| fig-align: "center"
#| dpi: 150
from pathlib import Path
from PIL import Image
import matplotlib.pyplot as plt

# Load image
img_path = Path().resolve().parent.parent / "results" /"plots" / "fee_distribution.png"
img = Image.open(img_path)

# Display
plt.figure(figsize=(8, 5))
plt.imshow(img)
plt.axis('off')
plt.tight_layout()
plt.show()
```

To mitigate this, we applied a logarithmic transformation to fastestFee, which compresses large values and helps stabilize variance across the seriesâ€”making it more suitable for forecasting. Note that not all models require this transformation, we applied it selectively based on model needs.

To reduce noise and enhance short-term signal clarity, we resampled the original 5-minute data into 15-minute intervals. Extremely short intervals introduce high-frequency noise, making patterns harder for models to learn, while overly long intervals risk losing short-term dynamics. As resampling effectively reduces the number of data points, it eases the computational burden during hyperparameter tuning and model training, enabling more efficient use of resources. To strike a balance, we tested multiple resampling frequencies and evaluated their predictive strength using the decay ratio.

```{python}
#| label: fig-decay-ratio
#| fig-cap: "Decay ratio by sampling interval. Higher ratios indicate stronger AR(1)-like structure and better short-term predictability."
#| fig-align: "center"
#| dpi: 150

from pathlib import Path
from PIL import Image
import matplotlib.pyplot as plt

# Load image
img_path = Path().resolve().parent.parent / "results" /"plots" / "optimal_interval.png"
img = Image.open(img_path)

# Display
plt.figure(figsize=(8, 5))
plt.imshow(img)
plt.axis('off')
plt.tight_layout()
plt.show()
```

As shown in @fig-decay-ratio, the 15-minute interval achieved the highest decay ratio, indicating the strongest short-term autocorrelation structure. This supports its use as the default sampling interval in our modeling pipeline.

Finally, we created lagged variables and rolling aggregates to help models capture temporal dependencies. The final feature set retained most original variables except those flagged for leakage, balancing completeness with modeling integrity.
