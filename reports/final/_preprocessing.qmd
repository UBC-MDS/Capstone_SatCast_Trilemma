```{python}
import sys
from pathlib import Path
from scipy.stats import pearsonr

project_root = Path().resolve().parent.parent
src_path = project_root / "src"
sys.path.append(str(src_path))
from preprocess_raw_parquet import preprocess_raw_parquet

df = preprocess_raw_parquet('../../data/raw/mar_5_may_12.parquet')
x = df["recommended_fee_halfHourFee"].dropna()
y = df["recommended_fee_fastestFee"].dropna()
common_idx = x.index.intersection(y.index)
r, _ = pearsonr(x.loc[common_idx], y.loc[common_idx])
```

## Feature Preprocessing

We conducted exploratory correlation analysis to understand the data and validate feature relevance. While not used for formal feature selection, the analysis confirmed that several predictors carry meaningful signals for the target variable, *fastestFee*.

Moreover, we excluded features with extremely high correlation to the target to prevent data leakage. For instance, *halfHourFee* showed a correlation of `{python} f"{r:.2f}"` with *fastestFee*. These features were removed from all models except DeepAR and TFT, where architectural constraints made exclusion more difficult.

We also applied several feature engineering steps. The distribution of *fastestFee* was highly right skewed, with most values concentrated at the low end and a few extreme spikes (@fig-fastestfee-distribution). This skew can reduce model stability and violate assumptions in some forecasting methods. 

```{python}
#| label: fig-fastestfee-distribution
#| fig-cap: "The distribution of fastestFee shows strong right skew."
#| fig-align: "center"
#| dpi: 150
from pathlib import Path
from PIL import Image
import matplotlib.pyplot as plt

# Load image
img_path = Path().resolve().parent.parent / "results" /"plots" / "fee_distribution.png"
img = Image.open(img_path)

# Display
plt.figure(figsize=(8, 5))
plt.imshow(img)
plt.axis('off')
plt.tight_layout()
plt.show()
```

To address this, we selectively applied a logarithmic transformation to *fastestFee* to compress large values, stabilize variance, and make the series more suitable for modeling. 

Furthermore, we resampled the original 5-minute data into 15-minute intervals to reduce noise and improve short-term signal clarity. Excessively short intervals introduce high-frequency noise, while overly long intervals risk losing important dynamics. Resampling also reduces the data volume, easing the computational load during tuning and training. To find the right balance, we tested multiple frequencies and assessed their predictive strength using the decay ratio.

```{python}
#| label: fig-decay-ratio
#| fig-cap: "Decay ratio by sampling interval. Higher ratios indicate stronger AR(1)-like structure and better short-term predictability."
#| fig-align: "center"
#| dpi: 150

from pathlib import Path
from PIL import Image
import matplotlib.pyplot as plt

# Load image
img_path = Path().resolve().parent.parent / "results" /"plots" / "optimal_interval.png"
img = Image.open(img_path)

# Display
plt.figure(figsize=(8, 5))
plt.imshow(img)
plt.axis('off')
plt.tight_layout()
plt.show()
```

As shown in @fig-decay-ratio, the 15-minute interval yielded the highest decay ratio, indicating the strongest short-term autocorrelation. This supported its selection as the default sampling interval.

Finally, we created lagged features and rolling aggregates to help models capture temporal dependencies. The final feature set retained most original variables, excluding those flagged for leakage, to balance completeness and modeling integrity.
