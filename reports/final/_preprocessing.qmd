## Feature Preprocessing

To prepare the data, we conducted exploratory correlation analysis between features and the target variable fastestFee, which confirmed that several predictors contained useful signals. This was not used for formal feature selection, but helped validate feature relevance.

To prevent target leakage, we removed a few features that had extremely high correlation with the target (e.g., halfHourFee, exhibiting a 0.98 correlation with fastestFee). These were excluded from all models except DeepAR and TFT, where model constraints made removal more complex.

We also applied several feature engineering steps. We first examined the distribution of fastestFee and found it to be highly right-skewed, with most values clustered at the low end and a few extreme spikes shown in @fig-fastestfee-distribution. This heavy skew can hinder model stability and violate common assumptions.

```{python}
#| label: fig-fastestfee-distribution
#| fig-cap: "The distribution of fastestFee shows strong right skew."
#| fig-align: "center"
#| dpi: 150
from pathlib import Path
from PIL import Image
import matplotlib.pyplot as plt

# Load image
img_path = Path().resolve().parent.parent / "results" /"plots" / "fee_distribution.png"
img = Image.open(img_path)

# Display
plt.figure(figsize=(8, 5))
plt.imshow(img)
plt.axis('off')
plt.tight_layout()
plt.show()
```

To mitigate this, we applied a logarithmic transformation to fastestFee, which compresses large values and helps stabilize variance across the seriesâ€”making it more suitable for forecasting.

To reduce noise and enhance short-term signal clarity, we resampled the original 5-minute data into 15-minute intervals. Extremely short intervals introduce high-frequency noise, making patterns harder for models to learn, while overly long intervals risk losing short-term dynamics. As resampling effectively reduces the number of data points, it eases the computational burden during hyperparameter tuning and model training, enabling more efficient use of resources. To strike a balance, we tested multiple resampling frequencies and evaluated their predictive strength using the decay ratio.

```{python}
#| label: fig-decay-ratio
#| fig-cap: "Decay ratio by sampling interval. Higher ratios indicate stronger AR(1)-like structure and better short-term predictability."
#| fig-align: "center"
#| dpi: 150

from pathlib import Path
from PIL import Image
import matplotlib.pyplot as plt

# Load image
img_path = Path().resolve().parent.parent / "results" /"plots" / "optimal_interval.png"
img = Image.open(img_path)

# Display
plt.figure(figsize=(8, 5))
plt.imshow(img)
plt.axis('off')
plt.tight_layout()
plt.show()
```

As shown in @fig-decay-ratio, the 15-minute interval achieved the highest decay ratio, indicating the strongest short-term autocorrelation structure. This supports its use as the default sampling interval in our modeling pipeline.

Finally, we created lagged variables and rolling aggregates to help models capture temporal dependencies. The final feature set retained most original variables except those flagged for leakage, balancing completeness with modeling integrity.
