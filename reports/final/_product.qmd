This section presents results from our analysis, including model performance comparisons and forecast visualizations, followed by a discussion of the final data product—its intended users, applications, and extensibility.

## Results: Model Performance and Forecast Visualization
Model comparison tables and the forecast plots illustrate how predictive fidelity improves as we progress from classical statistics to deep learning.
Baselines such as HWES and SARIMA track the day-level seasonal drift but miss the sharp dips and spikes that dominate the fee landscape, as shown in @fig-hwes-sarima.
```{python}
project_root = Path().resolve().parent.parent  # or adjust as needed
src_path = project_root / "src"
sys.path.append(str(src_path))

from plot_forecast_comparison import plot_forecast_comparison
```
```{python}
#| label: fig-hwes-sarima
#| fig-cap: "HWES and SARIMA forecasts vs actual fee (test set)"
#| fig-align: "center"
#| dpi: 150

fig, ax = plot_forecast_comparison(
    forecasts["HWES"], "Forecast (HWES)",
    forecasts["SARIMA"], "Forecast (SARIMA)"
)
plt.show()
```

Prophet, with its flexible trend and built-in seasonality terms, improves the global fit but still smooths over most intraday spikes, 
yielding a custom loss of about `{python} f"{metrics_df.loc['custom_loss', 'Prophet']:.2f}"`
and an RMSE just about `{python} f"{metrics_df.loc['rmse', 'Prophet']:.2f}"`. XGBoost, which ingests engineered lags and mempool signals, 
pushes average error lower than any of the purely statistical models. 
This model yields an RMSE of `{python} f"{metrics_df.loc['rmse', 'XGBoost']:.2f}"`, but continues to understate high-frequency volatility,
as shown in @fig-prophet-xgboost. The custom loss of XGBoost is `{python} f"{metrics_df.loc['custom_loss', 'XGBoost']:.2f}"`, indicating that while the model captures general trends, it still struggles to fully account for the sharp fee spikes and intraday volatility present in the data.
```{python}
#| label: fig-prophet-xgboost
#| fig-cap: "Prophet and XGBoost forecasts vs actual fee (test set)"
#| fig-align: "center"
#| dpi: 150

fig, ax = plot_forecast_comparison(
    forecasts["Prophet"], "Forecast (Prophet)",
    forecasts["XGBoost"], "Forecast (XGBoost)",
    color1="orange", color2="purple"
)
plt.show()

```

The neural models represent a meaningful shift in modeling capacity.
While DeepAR introduces sequential awareness through recurrence, it falls short of Prophet and XGBoost in short-term fee tracking, with a custom loss of `{python} f"{metrics_df.loc['custom_loss', 'DeepAR']:.2f}"` and RMSE of `{python} f"{metrics_df.loc['rmse', 'DeepAR']:.2f}"`.
It is the TFT that best synchronises with both the amplitude and timing of sudden fee surges (darkcyan versus black traces in @fig-deepar-tft). 

```{python}
#| label: fig-deepar-tft
#| fig-cap: "DeepAR and TFT forecasts vs actual fee (test set)"
#| fig-align: "center"
#| dpi: 150

fig, ax = plot_forecast_comparison(
    forecasts["DeepAR"], "Forecast (DeepAR)",
    forecasts["TFT"], "Forecast (TFT)",
    color1="crimson",     # DeepAR
    color2="darkcyan"     # TFT
)
plt.show()
```
Quantitatively, TFT reduces the bespoke volatility-aware loss from about `{python} f"{metrics_df.loc['custom_loss', 'XGBoost']:.2f}"`, the lowest among the baselines, to `{python} f"{metrics_df.loc['custom_loss', 'TFT']:.2f}"`,
representing a `{python} f"{100 * (metrics_df.loc['custom_loss', 'XGBoost'] - metrics_df.loc['custom_loss', 'TFT']) / metrics_df.loc['custom_loss', 'XGBoost']:.0f}%"` improvement.
It also brings RMSE down from the `{python} f"{metrics_df.loc['rmse', 'XGBoost']:.2f}"`–`{python} f"{metrics_df.loc['rmse', 'HWES']:.2f}"` range to `{python} f"{metrics_df.loc['rmse', 'TFT']:.2f}"`,
a relative gain of `{python} f"{100 * (min(metrics_df.loc['rmse', ['Prophet', 'XGBoost']]) - metrics_df.loc['rmse', 'TFT']) / min(metrics_df.loc['rmse', ['Prophet', 'XGBoost']]):.0f}%"`.
MAE, MAPE, and distribution-shape penalties follow the same pattern, confirming that TFT strikes the strongest balance between overall accuracy and sensitivity to congestion-driven shocks, as shown in the comprehensive comparison in @tbl-metrics.

```{python}
#| label: tbl-metrics
#| tbl-cap: Model performance on test data.
from IPython.display import Markdown 
core_metrics = metrics_df.loc[["custom_loss", "rmse", "mae", "mape"]].round(2)
sorted_cols = core_metrics.loc["custom_loss"].sort_values().index
core_metrics_sorted = core_metrics[sorted_cols]
Markdown(core_metrics_sorted.round(2).to_markdown(index=True))
```

## Data Product Overview

The data product directly supports Trilemma Capital’s mission of serving industry talent and advancing Bitcoin infrastructure through data science, both educational and technical. Its design intentionally tailors to three core audiences. General users and institutions can rely on the 24-hour forecasts to plan transactions and reduce fee costs. Learners and educators receive a transparent, step-by-step walkthrough of Bitcoin-fee forecasting and time-series methodology. Industry experts and partners see infrastructure-grade modeling practice embodied in a modular pipeline and well-documented repository. 

The product is purposefully modular. Jupyter notebooks guide users through EDA, modeling decisions, and final TFT results. Python scripts implement a structured pipeline for reproducible experiments and easy re-training on new data. Finally, the open-source GitHub repository—with clear documentation—enables collaboration, scalability, and long-term extensibility.

## Value Proposition and Justification

Our product addresses the key limitation of current Bitcoin fee tools, which are reactive, opaque, and confined to short-term horizons. Tools like estimatesmartfee and Mempool.space forecast fees only 10 - 60 minutes ahead, offering single-rate suggestions without transparency into volatility drivers or user-specific needs. In contrast, our system produces 24-hour forecasts across multiple urgency tiers, helping users align timing with cost tolerance. We chose Jupyter notebooks to explain modeling decisions and trade-offs in an accessible format. Modular pipeline scripts offer reproducibility and extensibility for advanced users, while an open-source GitHub repository invites transparency and community contribution—aligned with our partner’s infrastructure mission.

## Product Strengths and Limitations

The product balances analytical depth with real-world usability. It offers tiered, volatility-aware forecasts that are actionable for users ranging from individuals to technical infrastructure teams. Tiered fee outputs (fastest, economy, minimum) accommodate different urgency and cost preferences, offering flexibility not found in existing one-size-fits-all tools. On the development side, its modular structure promotes extensibility—developers can swap models, tune hyperparameters, or introduce new features without disrupting the pipeline. Full transparency also enables peers to audit modeling assumptions, reproduce visualizations, and verify performance results. The inclusion of a custom loss function aligns model behavior with real-world fee dynamics, supporting better planning around cost and urgency trade-offs.

Nonetheless, the system has limitations that affect both performance and accessibility. From a usability standpoint, deep learning models like TFT demand significant compute resources and are not easily accessible to users without GPUs. More critically, model performance will degrade over time unless retrained regularly to adapt to shifting network dynamics. On the input side, models currently rely on lagging indicators and do not incorporate real-time external signals—such as exchange outflows or policy announcements—making them less responsive to abrupt market events that trigger sudden fee spikes. Additionally, the absence of uncertainty estimates may limit users’ confidence in edge cases, and the lack of a real-time dashboard or API could restrict adoption by users expecting more interactive or integrated experiences.

## Design Trade-Offs

We made design choices that prioritize flexibility over complexity. For example, we avoided creating a Makefile to run all models at once due to compute constraints. Probabilistic outputs were also omitted, as current magnitude forecasts are too noisy to support reliable uncertainty estimates. Likewise, real-time API deployment was ruled out due to cost and performance constraints of transformer-based models. Instead, the product supports batch forecasts and modular experimentation as a foundation for future enhancements, such as adaptive loss tuning, hybrid model pipelines, or more scalable delivery formats.
