This section presents model performance results and forecast visualizations, followed by a discussion of the final data product—its deliverables, target users, use cases, and extensibility.

## Results: Model Performance and Forecast Visualization
We evaluated six forecasting models—HWES, SARIMA, Prophet, XGBoost, DeepAR, and Temporal Fusion Transformer (TFT)—for their effectiveness in capturing short-term Bitcoin transaction fee dynamics, particularly high-frequency volatility. For baseline comparison, a global median model is also included, as summarized in @tbl-metrics.

We evaluated six forecasting models—HWES, SARIMA, Prophet, XGBoost, DeepAR, and TFT—for their ability to capture next day (24-hour) Bitcoin fee dynamics, with a focus on high-frequency volatility. We also included a global median model for baseline comparison, as summarized in @tbl-metrics.

```{python}
from IPython.display import Markdown, display
core_metrics = metrics_df.loc[["custom_loss", "rmse", "mape"]].round(3)
sorted_cols = core_metrics.loc["custom_loss"].sort_values().index
core_metrics_sorted = core_metrics[sorted_cols]
```
```{python}
#| label: tbl-metrics
#| tbl-cap: Model performance on test data.
from tabulate import tabulate
display(Markdown(core_metrics_sorted.round(3).to_markdown(index=True)))
```

We used HWES and SARIMA as simple, fast-to-train baselines for coarse fee trend forecasting. Both captured broad day-level seasonality but missed sharp intraday dips and spikes (see @fig-hwes-sarima). HWES recorded a custom loss of `{python} f"{metrics_df.loc['custom_loss', 'HWES']:.2f}"`, RMSE of `{python} f"{metrics_df.loc['rmse', 'HWES']:.2f}"`, and MAPE of `{python} f"{metrics_df.loc['mape', 'HWES']:.2f}"`. SARIMA performed similarly (`{python} f"{metrics_df.loc['custom_loss', 'SARIMA']:.2f}"` custom loss, `{python} f"{metrics_df.loc['rmse', 'SARIMA']:.2f}"` RMSE, `{python} f"{metrics_df.loc['mape', 'SARIMA']:.2f}"` MAPE). Both models failed to improve on the global median baseline, which posted `{python} f"{metrics_df.loc['custom_loss', 'Median']:.2f}"` custom loss, `{python} f"{metrics_df.loc['rmse', 'Median']:.2f}"` RMSE, and `{python} f"{metrics_df.loc['mape', 'Median']:.2f}"` MAPE.

```{python}
project_root = Path().resolve().parent.parent  # or adjust as needed
src_path = project_root / "src"
sys.path.append(str(src_path))

from plot_forecast_comparison import plot_forecast_comparison
```
```{python}
#| label: fig-hwes-sarima
#| fig-cap: "HWES and SARIMA forecasts vs actual fee on test data"
#| fig-align: "center"
#| dpi: 150

fig, ax = plot_forecast_comparison(
    forecasts["HWES"], "Forecast (HWES)",
    forecasts["SARIMA"], "Forecast (SARIMA)"
)
plt.show()
```

We used Prophet for its simplicity, interpretability, and built-in support for trend shifts, seasonality, and holiday effects—features well-suited to capturing macro-level fee patterns with minimal tuning. However, it oversmoothed sharp fee fluctuations and produced the highest MAPE (`{python} f"{metrics_df.loc['mape', 'Prophet']:.2f}"`) among all models. Its custom loss (`{python} f"{metrics_df.loc['custom_loss', 'Prophet']:.2f}"`) and RMSE (`{python} f"{metrics_df.loc['rmse', 'Prophet']:.2f}"`) placed it only marginally ahead of the global median (`{python} f"{metrics_df.loc['custom_loss', 'Median']:.2f}"`, `{python} f"{metrics_df.loc['rmse', 'Median']:.2f}"`), highlighting its limited advantage. As shown in @fig-prophet-xgboost, its forecasts lagged behind actual spikes and decayed slowly.

We selected XGBoost to model nonlinear interactions between features like mempool congestion and lagged fees. It slightly outperformed Prophet, with a custom loss of `{python} f"{metrics_df.loc['custom_loss', 'XGBoost']:.2f}"`, RMSE of `{python} f"{metrics_df.loc['rmse', 'XGBoost']:.2f}"`, and MAPE of `{python} f"{metrics_df.loc['mape', 'XGBoost']:.2f}"`, but remained on par with classical baselines. It tended to underestimate sharp fee jumps and often produced flat or conservative predictions in volatile regions (@fig-prophet-xgboost).


```{python}
#| label: fig-prophet-xgboost
#| fig-cap: "Prophet and XGBoost forecasts vs actual fee on test data"
#| fig-align: "center"
#| dpi: 150

fig, ax = plot_forecast_comparison(
    forecasts["Prophet"], "Forecast (Prophet)",
    forecasts["XGBoost"], "Forecast (XGBoost)",
    color1="orange", color2="purple"
)
plt.show()

```

The neural models marked a meaningful shift in modeling capacity, enabling the system to learn from richer temporal patterns and nonlinear interactions. We used DeepAR to capture sequential dependencies through autoregressive recurrence. It provided a path toward probabilistic forecasting and outperformed all classical models with a custom loss of `{python} f"{metrics_df.loc['custom_loss', 'DeepAR']:.2f}"` and RMSE of `{python} f"{metrics_df.loc['rmse', 'DeepAR']:.2f}"`.

We chose TFT for its architecture, which combines attention mechanisms, gating, and variable selection. This design enabled it to track both the magnitude and timing of sudden fee surges. TFT delivered the strongest performance on high-frequency volatility, making it well-suited for fine-grained, urgency-tiered fee forecasts with real-time planning value (see @fig-deepar-tft).

```{python}
#| label: fig-deepar-tft
#| fig-cap: "DeepAR and TFT forecasts vs actual fee on test data"
#| fig-align: "center"
#| dpi: 150

fig, ax = plot_forecast_comparison(
    forecasts["DeepAR"], "Forecast (DeepAR)",
    forecasts["TFT"], "Forecast (TFT)",
    color1="crimson",     # DeepAR
    color2="darkcyan"     # TFT
)
plt.show()
```

Quantitatively, TFT reduced the bespoke volatility-aware loss from approximately `{python} f"{custom_min_baseline:.2f}"`—the best among non-neural models—to `{python} f"{custom_loss_tft:.2f}"`, representing a `{python} f"{custom_loss_improvement_pct:.0f}%"` improvement. It also lowered RMSE from the `{python} f"{rmse_min_baseline:.2f}"`–`{python} f"{rmse_max_baseline:.2f}"` range to `{python} f"{rmse_tft:.2f}"`, yielding a `{python} f"{rmse_improvement_pct:.0f}%"` relative gain. MAPE followed the same trend, reaching `{python} f"{mape_tft:.2f}"` and marking a meaningful `{python} f"{mape_improvement_pct:.0f}%"` gain in relative accuracy. Together, the results show that TFT achieved the strongest balance of accuracy and responsiveness to congestion-driven fee spikes.


## Data Product Overview

The data product consists of three core components: Jupyter notebooks that guide users through EDA, modeling, and findings; Python scripts that define a structured pipeline for reproducible training; and an open-source GitHub repository with clear documentation to support collaboration and long-term extensibility.

Its design is intentionally modular to serve diverse use cases. General users and institutions can rely on the 24-hour forecasts to plan transactions and reduce fee costs. Learners and educators engage with a transparent, step-by-step forecasting walkthrough. Technical developers and our partner benefit from a scalable, reproducible pipeline and workflow.

## Value Proposition and Strengths

Beyond addressing the analytical limitation of forecast horizon by extending it from 10 to 60 minutes to a full 24 hours, we introduced forecasts across multiple urgency tiers (fastest, economy, minimum), giving users actionable insight aligned with cost sensitivity and transaction timing. We also delivered significant value on the development side by emphasizing transparency, modularity, and extensibility. Our scripts, notebooks, and open-source repository enhance accessibility through clear narratives, offer developers greater customization flexibility, and enable community contribution—advancing open infrastructure-grade Bitcoin research in line with our partner’s mission.

## Limitations and Design Trade-Offs

Several constraints affect both product's performance and accessibility. TFT powers the pipeline but raises entry barriers, often requiring GPUs or cloud resources. Regular retraining will be necessary as network conditions evolve, but automation is best deferred until model accuracy stabilizes. Confidence intervals and real-time APIs are valuable but not yet justified due to forecast noise and current development priorities. Reactive inputs like mempool congestion and block composition limit foresight and slow model maturity. To remain agile, the script-first design supports rapid iteration, modular updates, and future extensions such as adaptive loss tuning, hybrid pipelines, and scalable deployment.