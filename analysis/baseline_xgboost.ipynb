{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "568b0da4-758f-49c0-947e-f45bd759f7e1",
   "metadata": {},
   "source": [
    "# Baseline Model: XGBoost\n",
    "In this notebook, we forecast the fastest Bitcoin transaction fee (sats/vByte) using XGBoost Regressor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb446c7a-6d0b-4575-9691-d3107a299c3d",
   "metadata": {},
   "source": [
    "### Why We Chose XGBoost as a Baseline Model\n",
    "We selected XGBoost as one of our baseline models because it provides a strong balance between performance, interpretability, and efficiency when working with structured tabular data. Unlike traditional linear models, XGBoost can effectively capture non-linear relationships and complex interactions between features, which includes both temporal patterns (via lag features) and contextual signals (like mempool and market conditions). \n",
    "\n",
    "Our target variable recommended_fee_fastestFee is not only time-dependent but also correlates with other recommended fee rates (e.g., halfHourFee, hourFee, etc.), which serve as important external features. When there are known correlations between the target and input features in the presence of redundancy or nonlinearity, XGBoost offers both high performance and interpretability. The exploratory data analysis confirms these relationships and reveals substantial variability and temporal structure in the data, which makes simpler linear models less effective. XGBoost, in contrast, is robust to multicollinearity, handles missing data gracefully, and naturally incorporates lag features. Additionally, it provides useful feature importance insights and performs well with minimal preprocessing, which makes it a practical and reliable starting point for benchmarking more complex forecasting models.\n",
    "\n",
    "By tuning key hyperparameters and evaluating the model with MAE and RMSE, we can gauge how well tree-based methods handle fee rate prediction and set a meaningful performance benchmark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486a8f9c-2562-4ab5-9298-5ac070aab29b",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5035696-c553-40c0-a8a2-d77dd3395da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, make_scorer, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib  \n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "import seaborn as sns\n",
    "from sktime.forecasting.compose import make_reduction\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split, ForecastingRandomizedSearchCV\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from sktime.performance_metrics.forecasting import mean_absolute_error as mean_absolute_error_sktime\n",
    "from sktime.split import SlidingWindowSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433b9397-9364-4204-85f9-07db35e47953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from XGBoost import data_split, build_random_search, create_lag_features_fast,evaluate_model,plot_result,evaluate_best_model\n",
    "from preprocess_raw_parquet import preprocess_raw_parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4205e1-608a-4f4b-b36e-320403f4300c",
   "metadata": {},
   "source": [
    "### Load data\n",
    "To ensure that we only train on complete data, we drop the lag rows which contain NaN values introduced by lag feature creation. Because of spikes in the last day, we remove the last 24-hour from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcc5012-79ec-497f-bd53-c5bd6fc83d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess_raw_parquet(\"../data/raw/mar_5_may_12.parquet\")\n",
    "df.dropna(inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40b9bec",
   "metadata": {},
   "source": [
    "### Create lag features\n",
    "\n",
    "We will need lagging feature for XGboost. Since each hour has 4 data points (60 ÷ 15), 48 hours = 192 lag steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e026a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = range(1, 193)  # 48 hours of 15-minute intervals\n",
    "df = create_lag_features_fast(df, 'recommended_fee_fastestFee', lags)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908d8217-abcd-4567-a9b2-91086baa15c4",
   "metadata": {},
   "source": [
    "## Optimization - Find the best params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8d1178-9f21-4a21-86a5-dc12720e2dd7",
   "metadata": {},
   "source": [
    "### Params\n",
    "We choose RandomizedSearch to optimize the model. It allows us to explore a broader space efficiently with fewer iterations. Here are the ranges of parameters.\n",
    "\n",
    "n_estimators: [50, 100, 150] \n",
    "It controls the number of boosting rounds. Small values (50) may underfit, while higher values (150) may lead to better performance but risk overfitting. Keeping this modest helps reduce training time and keeps the search space focused.\n",
    "\n",
    "max_depth: [1, 2, 3] \n",
    "It controls the maximum depth of individual trees. Shallow trees (1–3) are less likely to overfit and are more interpretable. Especially appropriate when the number of features is high and the time resolution is short (5-min data), helping avoid overly complex trees.\n",
    "\n",
    "learning_rate: [0.01, 0.05, 0.1] \n",
    "Learning rate shrinks the contribution of each tree. Lower values slow down learning but improve generalization. Chosen to strike a balance between convergence speed and model stability.\n",
    "\n",
    "subsample: [0.6, 0.8, 0.9] \n",
    "Fraction of rows used per tree. It introduces randomness ,helping prevent overfitting and improving robustness. 0.6–0.9 allows experimentation with more regularized models.\n",
    "\n",
    "colsample_bytree: [0.6, 0.8, 0.9] \n",
    "Fraction of columns (features) used to build each tree. Especially helpful when there are many features. It helps prevent overfitting by decorrelating trees.\n",
    "\n",
    "gamma: [1, 3, 5] \n",
    "Minimum loss reduction to make a split. It acts as a regularizer to control tree growth. Higher values force the model to make only meaningful splits.\n",
    "\n",
    "reg_lambda: [5, 10, 20] \n",
    "L2 regularization on leaf weights. It prevents overly large weights, stabilizing the model especially with correlated features. It also helps handle multicollinearity and improve generalization.\n",
    "\n",
    "reg_alpha: [5, 10, 20] \n",
    "L1 regularization (sparsity). It encourages feature selection by zeroing out less useful ones, and is particularly useful in high-dimensional datasets (e.g., many lag features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c4db9e-bade-4bb1-b51e-4cdd46e8d413",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'estimator__n_estimators': [50, 100, 150],\n",
    "    'estimator__max_depth': [1, 2, 3],\n",
    "    'estimator__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'estimator__subsample': [0.6, 0.8, 0.9],\n",
    "    'estimator__colsample_bytree': [0.6, 0.8, 0.9],\n",
    "    'estimator__gamma': [1, 3, 5],\n",
    "    'estimator__reg_lambda': [5, 10, 20],\n",
    "    'estimator__reg_alpha': [5, 10, 20]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daa243e-4634-4e82-9eb7-9594d49986d0",
   "metadata": {},
   "source": [
    "### Sliding window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d8318a-a76d-4c82-a04b-e0f8357b4470",
   "metadata": {},
   "source": [
    "The dataset contains 10-week data. Use each of the five weeks data to train the model. Fixed-size window helps assess how well model generalizes across changing conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d4030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fh = ForecastingHorizon(np.arange(1, 97), is_relative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818074ab-48cb-4777-b281-ad8ad233b0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_per_week, avg_metrics,y_test,y_pred = evaluate_model(df,param_dist,interval=15,weeks=10,fh=fh,sliding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3843fa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAverage metrics over all weeks:\")\n",
    "for k, v in avg_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f1b30f-e538-4108-92ef-762f6619ea49",
   "metadata": {},
   "source": [
    "### Plot \n",
    "\n",
    "We plot the predicted fee rates and the actual ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c459e9-85d2-464d-84a0-170407305cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(y_test=y_test,y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f7fa2b-5b06-498b-8163-2e6c6ed96c46",
   "metadata": {},
   "source": [
    "## Expanding window\n",
    "\n",
    "The dataset contains 10-week data. Start with the first week and train the model five times by progressively adding one more week each time. This methods simulates real-world scenario as more historical data becomes available and tests whether model improves as it learns from a growing dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64e1bc3-7b49-4485-8bd6-a1e2b9908828",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_per_week, avg_metrics,y_test,y_pred = evaluate_model(df,param_dist,interval=15,weeks=10,fh=fh,sliding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f8d4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAverage metrics over all weeks:\")\n",
    "for k, v in avg_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75300102-dbf7-404c-bb01-8c209631a020",
   "metadata": {},
   "source": [
    "### Plot \n",
    "\n",
    "We plot the predicted fee rates and the actual ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f063a8-4b99-4992-96d0-84cd2cc583ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(y_test=y_test,y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17001558",
   "metadata": {},
   "source": [
    "### The best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97790f8",
   "metadata": {},
   "source": [
    "Fianlly, we try to train with expanding window on the whole data set. Because of spikes in the last day, we remove the last 24-hour from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf32768",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.iloc[:-96]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95d798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics,y_test,y_pred = evaluate_best_model(df,param_dist,15,fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1e1981",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMetrics:\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fb18dd",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fbc652",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(y_test=y_test,y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26e138b-4a41-4312-9ee6-2957747b2715",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Based on the average metrics, expanding window performs better than sliding window across all metrics. Moreover, it slightly outperforms baseline (i.e. global median) within the 5-weeks data available. However, the performance is still far from ideal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
