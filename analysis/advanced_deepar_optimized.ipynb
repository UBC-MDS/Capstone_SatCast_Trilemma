{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdb326fa-9d8c-41b0-a9ff-eec3fc2071cc",
   "metadata": {},
   "source": [
    "## Deepar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4527da80-acc7-4c5f-817a-4f8810565f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.torch.model.deepar import DeepAREstimator\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import islice\n",
    "from gluonts.evaluation import Evaluator\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d78d0788-dd97-421b-9e79-99d1801e6e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSONL into a list of dicts\n",
    "with open(\"../data/processed/deepar_dataset.jsonl\") as f:\n",
    "    series_list = [json.loads(line) for line in f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d0f6961-b14c-4697-adcc-cce83dc22248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset 100 data points\n",
    "s = series_list[0]\n",
    "dataset = ListDataset(series_list, freq=\"5min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073aafbc-5611-4cff-b353-990e09095acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.torch.model.deepar import DeepAREstimator\n",
    "\n",
    "estimator = DeepAREstimator(\n",
    "    prediction_length=288,\n",
    "    context_length=288,\n",
    "    freq=\"5min\",\n",
    "    lags_seq=[1, 2, 3, 6, 12, 24, 48, 96, 192],\n",
    "    hidden_size=80,     # RNN cell size\n",
    "    num_layers=3,       # Depth of the network\n",
    "    dropout_rate=0.1,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-8,\n",
    "    batch_size=64,\n",
    "    num_batches_per_epoch=5,\n",
    "    trainer_kwargs={\n",
    "        \"max_epochs\": 10,\n",
    "        \"accelerator\": \"gpu\",\n",
    "        \"gradient_clip_val\": 10.0,\n",
    "        \"logger\": False,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1313048-1435-41be-adab-6da756bf6580",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\86166\\miniforge3\\envs\\563\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory c:\\Users\\86166\\Desktop\\Capstone\\Capstone_SatCast_Trilemma\\analysis\\checkpoints exists and is not empty.\n",
      "\n",
      "  | Name  | Type        | Params | Mode  | In sizes                                                       | Out sizes    \n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "0 | model | DeepARModel | 135 K  | train | [[1, 1], [1, 1], [1, 479, 6], [1, 479], [1, 479], [1, 288, 6]] | [1, 100, 288]\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "135 K     Trainable params\n",
      "0         Non-trainable params\n",
      "135 K     Total params\n",
      "0.544     Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cbf5a18131a4e05b399f68683fd7f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 50: 'train_loss' reached 1.31316 (best 1.31316), saving model to 'c:\\\\Users\\\\86166\\\\Desktop\\\\Capstone\\\\Capstone_SatCast_Trilemma\\\\analysis\\\\checkpoints\\\\epoch=0-step=50.ckpt' as top 1\n",
      "Epoch 1, global step 100: 'train_loss' reached 0.80176 (best 0.80176), saving model to 'c:\\\\Users\\\\86166\\\\Desktop\\\\Capstone\\\\Capstone_SatCast_Trilemma\\\\analysis\\\\checkpoints\\\\epoch=1-step=100.ckpt' as top 1\n",
      "Epoch 2, global step 150: 'train_loss' reached 0.60070 (best 0.60070), saving model to 'c:\\\\Users\\\\86166\\\\Desktop\\\\Capstone\\\\Capstone_SatCast_Trilemma\\\\analysis\\\\checkpoints\\\\epoch=2-step=150.ckpt' as top 1\n",
      "Epoch 3, global step 200: 'train_loss' reached 0.48669 (best 0.48669), saving model to 'c:\\\\Users\\\\86166\\\\Desktop\\\\Capstone\\\\Capstone_SatCast_Trilemma\\\\analysis\\\\checkpoints\\\\epoch=3-step=200.ckpt' as top 1\n",
      "Epoch 4, global step 250: 'train_loss' reached 0.20536 (best 0.20536), saving model to 'c:\\\\Users\\\\86166\\\\Desktop\\\\Capstone\\\\Capstone_SatCast_Trilemma\\\\analysis\\\\checkpoints\\\\epoch=4-step=250.ckpt' as top 1\n",
      "Epoch 5, global step 300: 'train_loss' was not in top 1\n",
      "Epoch 6, global step 350: 'train_loss' reached 0.12251 (best 0.12251), saving model to 'c:\\\\Users\\\\86166\\\\Desktop\\\\Capstone\\\\Capstone_SatCast_Trilemma\\\\analysis\\\\checkpoints\\\\epoch=6-step=350.ckpt' as top 1\n",
      "Epoch 7, global step 400: 'train_loss' reached 0.04164 (best 0.04164), saving model to 'c:\\\\Users\\\\86166\\\\Desktop\\\\Capstone\\\\Capstone_SatCast_Trilemma\\\\analysis\\\\checkpoints\\\\epoch=7-step=400.ckpt' as top 1\n",
      "Epoch 8, global step 450: 'train_loss' reached -0.17559 (best -0.17559), saving model to 'c:\\\\Users\\\\86166\\\\Desktop\\\\Capstone\\\\Capstone_SatCast_Trilemma\\\\analysis\\\\checkpoints\\\\epoch=8-step=450.ckpt' as top 1\n",
      "Epoch 9, global step 500: 'train_loss' reached -0.20553 (best -0.20553), saving model to 'c:\\\\Users\\\\86166\\\\Desktop\\\\Capstone\\\\Capstone_SatCast_Trilemma\\\\analysis\\\\checkpoints\\\\epoch=9-step=500.ckpt' as top 1\n",
      "Epoch 10, global step 550: 'train_loss' was not in top 1\n",
      "Epoch 11, global step 600: 'train_loss' was not in top 1\n",
      "Epoch 12, global step 650: 'train_loss' was not in top 1\n",
      "Epoch 13, global step 700: 'train_loss' was not in top 1\n",
      "Epoch 14, global step 750: 'train_loss' was not in top 1\n",
      "Epoch 15, global step 800: 'train_loss' was not in top 1\n",
      "Epoch 16, global step 850: 'train_loss' was not in top 1\n",
      "Epoch 17, global step 900: 'train_loss' was not in top 1\n",
      "Epoch 18, global step 950: 'train_loss' was not in top 1\n",
      "Epoch 19, global step 1000: 'train_loss' was not in top 1\n",
      "Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
      "Epoch 21, global step 1100: 'train_loss' was not in top 1\n",
      "Epoch 22, global step 1150: 'train_loss' reached -0.23113 (best -0.23113), saving model to 'c:\\\\Users\\\\86166\\\\Desktop\\\\Capstone\\\\Capstone_SatCast_Trilemma\\\\analysis\\\\checkpoints\\\\epoch=22-step=1150.ckpt' as top 1\n",
      "Epoch 23, global step 1200: 'train_loss' reached -0.31930 (best -0.31930), saving model to 'c:\\\\Users\\\\86166\\\\Desktop\\\\Capstone\\\\Capstone_SatCast_Trilemma\\\\analysis\\\\checkpoints\\\\epoch=23-step=1200.ckpt' as top 1\n",
      "Epoch 24, global step 1250: 'train_loss' reached -0.37660 (best -0.37660), saving model to 'c:\\\\Users\\\\86166\\\\Desktop\\\\Capstone\\\\Capstone_SatCast_Trilemma\\\\analysis\\\\checkpoints\\\\epoch=24-step=1250.ckpt' as top 1\n",
      "Epoch 25, global step 1300: 'train_loss' reached -0.41236 (best -0.41236), saving model to 'c:\\\\Users\\\\86166\\\\Desktop\\\\Capstone\\\\Capstone_SatCast_Trilemma\\\\analysis\\\\checkpoints\\\\epoch=25-step=1300.ckpt' as top 1\n",
      "Epoch 26, global step 1350: 'train_loss' was not in top 1\n",
      "Epoch 27, global step 1400: 'train_loss' was not in top 1\n",
      "Epoch 28, global step 1450: 'train_loss' was not in top 1\n",
      "Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
      "Epoch 30, global step 1550: 'train_loss' reached -0.43179 (best -0.43179), saving model to 'c:\\\\Users\\\\86166\\\\Desktop\\\\Capstone\\\\Capstone_SatCast_Trilemma\\\\analysis\\\\checkpoints\\\\epoch=30-step=1550.ckpt' as top 1\n",
      "Epoch 31, global step 1600: 'train_loss' was not in top 1\n",
      "Epoch 32, global step 1650: 'train_loss' was not in top 1\n",
      "Epoch 33, global step 1700: 'train_loss' reached -0.48154 (best -0.48154), saving model to 'c:\\\\Users\\\\86166\\\\Desktop\\\\Capstone\\\\Capstone_SatCast_Trilemma\\\\analysis\\\\checkpoints\\\\epoch=33-step=1700.ckpt' as top 1\n",
      "Epoch 34, global step 1750: 'train_loss' was not in top 1\n",
      "Epoch 35, global step 1800: 'train_loss' was not in top 1\n",
      "Epoch 36, global step 1850: 'train_loss' was not in top 1\n",
      "Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
      "Epoch 38, global step 1950: 'train_loss' was not in top 1\n",
      "Epoch 39, global step 2000: 'train_loss' was not in top 1\n",
      "Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
      "Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
      "Epoch 42, global step 2150: 'train_loss' was not in top 1\n",
      "Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
      "Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
      "Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
      "Epoch 46, global step 2350: 'train_loss' reached -0.61648 (best -0.61648), saving model to 'c:\\\\Users\\\\86166\\\\Desktop\\\\Capstone\\\\Capstone_SatCast_Trilemma\\\\analysis\\\\checkpoints\\\\epoch=46-step=2350.ckpt' as top 1\n",
      "Epoch 47, global step 2400: 'train_loss' was not in top 1\n",
      "Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
      "Epoch 49, global step 2500: 'train_loss' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "The classmethod `DeepARLightningModule.load_from_checkpoint` cannot be called on an instance. Please call it on the class type and make sure the return value is used.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\86166\\miniforge3\\envs\\563\\Lib\\site-packages\\gluonts\\torch\\model\\estimator.py:237\u001b[0m, in \u001b[0;36mPyTorchLightningEstimator.train\u001b[1;34m(self, training_data, validation_data, shuffle_buffer_length, cache_data, ckpt_path, **kwargs)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain\u001b[39m(\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    230\u001b[0m     training_data: Dataset,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PyTorchPredictor:\n\u001b[1;32m--> 237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle_buffer_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle_buffer_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpredictor\n",
      "File \u001b[1;32mc:\\Users\\86166\\miniforge3\\envs\\563\\Lib\\site-packages\\gluonts\\torch\\model\\estimator.py:213\u001b[0m, in \u001b[0;36mPyTorchLightningEstimator.train_model\u001b[1;34m(self, training_data, validation_data, from_predictor, shuffle_buffer_length, cache_data, ckpt_path, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m    206\u001b[0m     model\u001b[38;5;241m=\u001b[39mtraining_network,\n\u001b[0;32m    207\u001b[0m     train_dataloaders\u001b[38;5;241m=\u001b[39mtraining_data_loader,\n\u001b[0;32m    208\u001b[0m     val_dataloaders\u001b[38;5;241m=\u001b[39mvalidation_data_loader,\n\u001b[0;32m    209\u001b[0m     ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path,\n\u001b[0;32m    210\u001b[0m )\n\u001b[0;32m    212\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading best model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint\u001b[38;5;241m.\u001b[39mbest_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 213\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_model_path\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m TrainOutput(\n\u001b[0;32m    218\u001b[0m     transformation\u001b[38;5;241m=\u001b[39mtransformation,\n\u001b[0;32m    219\u001b[0m     trained_net\u001b[38;5;241m=\u001b[39mbest_model,\n\u001b[0;32m    220\u001b[0m     trainer\u001b[38;5;241m=\u001b[39mtrainer,\n\u001b[0;32m    221\u001b[0m     predictor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_predictor(transformation, best_model),\n\u001b[0;32m    222\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\86166\\miniforge3\\envs\\563\\Lib\\site-packages\\pytorch_lightning\\utilities\\model_helpers.py:121\u001b[0m, in \u001b[0;36m_restricted_classmethod_impl.__get__.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m is_scripting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjit\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m frameinfo\u001b[38;5;241m.\u001b[39mfilename \u001b[38;5;28;01mfor\u001b[39;00m frameinfo \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mstack())\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scripting:\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe classmethod `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` cannot be called on an instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please call it on the class type and make sure the return value is used.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    124\u001b[0m     )\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: The classmethod `DeepARLightningModule.load_from_checkpoint` cannot be called on an instance. Please call it on the class type and make sure the return value is used."
     ]
    }
   ],
   "source": [
    "predictor = estimator.train(dataset, ckpt_path=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63bc252-6629-4563-a18d-a46fc2299004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "forecast_it, ts_it = make_evaluation_predictions(\n",
    "    dataset=dataset,\n",
    "    predictor=predictor,\n",
    "    num_samples=100\n",
    ")\n",
    "forecast_list = list(forecast_it)\n",
    "ts_list = list(ts_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1b69e6-5878-43b4-9567-d20cc5f34b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running evaluation: 5it [00:00, 18.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregate Metrics:\n",
      "                 MSE: 1.4311\n",
      "           abs_error: 1045.1602\n",
      "      abs_target_sum: 2389.0000\n",
      "     abs_target_mean: 1.6590\n",
      "      seasonal_error: 0.8808\n",
      "                MASE: 0.9508\n",
      "                MAPE: 0.3061\n",
      "               sMAPE: 0.3974\n",
      "                MSIS: 12.4072\n",
      "num_masked_target_values: 0.0000\n",
      "   QuantileLoss[0.1]: 248.4927\n",
      "       Coverage[0.1]: 0.0035\n",
      "   QuantileLoss[0.2]: 469.2538\n",
      "       Coverage[0.2]: 0.1472\n",
      "   QuantileLoss[0.3]: 696.2949\n",
      "       Coverage[0.3]: 0.2444\n",
      "   QuantileLoss[0.4]: 901.0123\n",
      "       Coverage[0.4]: 0.3778\n",
      "   QuantileLoss[0.5]: 1045.1601\n",
      "       Coverage[0.5]: 0.4049\n",
      "   QuantileLoss[0.6]: 1161.5135\n",
      "       Coverage[0.6]: 0.4333\n",
      "   QuantileLoss[0.7]: 1225.4822\n",
      "       Coverage[0.7]: 0.4590\n",
      "   QuantileLoss[0.8]: 1221.7752\n",
      "       Coverage[0.8]: 0.5292\n",
      "   QuantileLoss[0.9]: 1108.9159\n",
      "       Coverage[0.9]: 0.6611\n",
      "                RMSE: 1.1963\n",
      "               NRMSE: 0.7211\n",
      "                  ND: 0.4375\n",
      "  wQuantileLoss[0.1]: 0.1040\n",
      "  wQuantileLoss[0.2]: 0.1964\n",
      "  wQuantileLoss[0.3]: 0.2915\n",
      "  wQuantileLoss[0.4]: 0.3772\n",
      "  wQuantileLoss[0.5]: 0.4375\n",
      "  wQuantileLoss[0.6]: 0.4862\n",
      "  wQuantileLoss[0.7]: 0.5130\n",
      "  wQuantileLoss[0.8]: 0.5114\n",
      "  wQuantileLoss[0.9]: 0.4642\n",
      "mean_absolute_QuantileLoss: 897.5445\n",
      "  mean_wQuantileLoss: 0.3757\n",
      "        MAE_Coverage: 0.2537\n",
      "                 OWA: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuximin/miniforge3/envs/satcast/lib/python3.12/site-packages/gluonts/json.py:102: UserWarning: Using `json`-module for json-handling. Consider installing one of `orjson`, `ujson` to speed up serialization and deserialization.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluation\n",
    "evaluator = Evaluator()\n",
    "agg_metrics, item_metrics = evaluator(ts_list, forecast_list)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nAggregate Metrics:\")\n",
    "for k, v in agg_metrics.items():\n",
    "    print(f\"{k:>20}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ea325f-626f-447a-85f1-2b1bbd3b3e37",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'forecast_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m n_plots \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mforecast_list\u001b[49m)  \u001b[38;5;66;03m# or set a lower limit for visual clarity\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_plots):\n\u001b[0;32m      3\u001b[0m     ts \u001b[38;5;241m=\u001b[39m ts_list[i]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'forecast_list' is not defined"
     ]
    }
   ],
   "source": [
    "n_plots = len(forecast_list)  # or set a lower limit for visual clarity\n",
    "for i in range(n_plots):\n",
    "    ts = ts_list[i]\n",
    "    forecast = forecast_list[i]\n",
    "\n",
    "    # Convert PeriodIndex to Timestamp for plotting\n",
    "    ts_index = ts.index.to_timestamp()\n",
    "\n",
    "    # Forecast timestamps\n",
    "    start = forecast.start_date.to_timestamp()\n",
    "    freq = pd.Timedelta(forecast.freq)\n",
    "    forecast_index = pd.date_range(start=start, periods=len(forecast.mean), freq=freq)\n",
    "\n",
    "    # Determine x-axis limits (last 2 days)\n",
    "    last_time = ts_index[-1]\n",
    "    xlim_start = last_time - pd.Timedelta(days=2)\n",
    "    xlim_end = last_time + pd.Timedelta(minutes=5 * len(forecast.mean))  # include forecast horizon\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.plot(ts_index, ts.values, label=\"True values\", color=\"black\")\n",
    "    plt.plot(forecast_index, forecast.mean, label=\"Forecast (mean)\", color=\"blue\")\n",
    "\n",
    "    plt.title(f\"Forecast vs True (Series {i})\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xlim(xlim_start, xlim_end)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea6114",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import optuna\n",
    "from gluonts.evaluation import Evaluator\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.model.deepar import DeepAREstimator\n",
    "from gluonts.trainer import Trainer\n",
    "\n",
    "def objective(trial):\n",
    "    hs = trial.suggest_categorical(\"hidden_size\", [40, 80, 120])\n",
    "    cl = trial.suggest_categorical(\"context_length\", [144, 288])\n",
    "    dr = trial.suggest_float(\"dropout_rate\", 0.0, 0.3)\n",
    "\n",
    "    estimator = DeepAREstimator(\n",
    "        prediction_length=prediction_length,\n",
    "        context_length=cl,\n",
    "        freq=freq,\n",
    "        hidden_size=hs,\n",
    "        dropout_rate=dr,\n",
    "        trainer=Trainer(epochs=5)\n",
    "    )\n",
    "\n",
    "    predictor = estimator.train(train_ds)\n",
    "\n",
    "    forecast_it, ts_it = make_evaluation_predictions(dataset=test_ds, predictor=predictor, num_samples=100)\n",
    "    agg_metrics, _ = Evaluator()(ts_it, forecast_it)\n",
    "\n",
    "    return agg_metrics[\"RMSE\"]\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "study.best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d17b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for forecast in forecasts:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    forecast.plot(color='g')\n",
    "    plt.plot(prediction_index, target[-prediction_length:], color='orange')\n",
    "    plt.fill_between(prediction_index, forecast.quantile(0.1), forecast.quantile(0.9), color='lightblue', alpha=0.4)\n",
    "    plt.title(\"Forecast with 80% Prediction Interval\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b89005",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "external_columns = ['mempool_blocks_nTx']\n",
    "exog_values = df[external_columns].T.values.tolist()\n",
    "\n",
    "train_ds = ListDataset([{\n",
    "    \"target\": df[target_column][:train_end_index].values,\n",
    "    \"start\": df.index[0],\n",
    "    \"feat_dynamic_real\": exog_values\n",
    "}], freq=freq)\n",
    "\n",
    "test_ds = ListDataset([{\n",
    "    \"target\": df[target_column].values,\n",
    "    \"start\": df.index[0],\n",
    "    \"feat_dynamic_real\": exog_values\n",
    "}], freq=freq)\n",
    "\n",
    "estimator = DeepAREstimator(\n",
    "    prediction_length=prediction_length,\n",
    "    context_length=288,\n",
    "    freq=freq,\n",
    "    use_feat_dynamic_real=True,\n",
    "    trainer=Trainer(epochs=5)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d355392",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "horizons = [72, 144, 288]\n",
    "results = []\n",
    "\n",
    "for horizon in horizons:\n",
    "    estimator = DeepAREstimator(\n",
    "        prediction_length=horizon,\n",
    "        context_length=horizon * 2,\n",
    "        freq=freq,\n",
    "        trainer=Trainer(epochs=5)\n",
    "    )\n",
    "    predictor = estimator.train(train_ds)\n",
    "    forecast_it, ts_it = make_evaluation_predictions(test_ds, predictor=predictor, num_samples=100)\n",
    "    agg_metrics, _ = Evaluator()(ts_it, forecast_it)\n",
    "    results.append((horizon, agg_metrics[\"RMSE\"], agg_metrics[\"MAE\"]))\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=[\"Horizon\", \"RMSE\", \"MAE\"])\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a09c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "split_points = list(range(5000, len(df), 288))\n",
    "metrics_expanding = []\n",
    "\n",
    "for split in split_points:\n",
    "    train_target = df[target_column][:split]\n",
    "    test_target = df[target_column][split:split+288]\n",
    "\n",
    "    train_ds = ListDataset([{\n",
    "        \"target\": train_target.values,\n",
    "        \"start\": df.index[0],\n",
    "        \"feat_dynamic_real\": [df[\"mempool_blocks_nTx\"].values]\n",
    "    }], freq=freq)\n",
    "\n",
    "    test_ds = ListDataset([{\n",
    "        \"target\": df[target_column].values,\n",
    "        \"start\": df.index[0],\n",
    "        \"feat_dynamic_real\": [df[\"mempool_blocks_nTx\"].values]\n",
    "    }], freq=freq)\n",
    "\n",
    "    estimator = DeepAREstimator(\n",
    "        prediction_length=288,\n",
    "        context_length=288,\n",
    "        freq=freq,\n",
    "        use_feat_dynamic_real=True,\n",
    "        trainer=Trainer(epochs=5)\n",
    "    )\n",
    "    predictor = estimator.train(train_ds)\n",
    "    forecast_it, ts_it = make_evaluation_predictions(test_ds, predictor=predictor, num_samples=100)\n",
    "    agg_metrics, _ = Evaluator()(ts_it, forecast_it)\n",
    "    metrics_expanding.append((df.index[split], agg_metrics[\"RMSE\"], agg_metrics[\"MAE\"]))\n",
    "\n",
    "metrics_expanding_df = pd.DataFrame(metrics_expanding, columns=[\"SplitTime\", \"RMSE\", \"MAE\"])\n",
    "metrics_expanding_df.plot(x=\"SplitTime\", y=[\"RMSE\", \"MAE\"], title=\"Expanding Window Evaluation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685185d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b079e4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59ee18f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2c9630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1839916c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87799594",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "563",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
