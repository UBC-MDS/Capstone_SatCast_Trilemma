{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdb326fa-9d8c-41b0-a9ff-eec3fc2071cc",
   "metadata": {},
   "source": [
    "## Deepar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4527da80-acc7-4c5f-817a-4f8810565f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.torch.model.deepar import DeepAREstimator\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import islice\n",
    "from gluonts.evaluation import Evaluator\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d78d0788-dd97-421b-9e79-99d1801e6e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSONL into a list of dicts\n",
    "with open(\"../data/processed/deepar_dataset.jsonl\") as f:\n",
    "    series_list = [json.loads(line) for line in f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d0f6961-b14c-4697-adcc-cce83dc22248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset 100 data points\n",
    "s = series_list[0]\n",
    "dataset = ListDataset(series_list, freq=\"5min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "073aafbc-5611-4cff-b353-990e09095acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.torch.model.deepar import DeepAREstimator\n",
    "\n",
    "estimator = DeepAREstimator(\n",
    "    prediction_length=288,\n",
    "    context_length=288,\n",
    "    freq=\"5min\",\n",
    "    lags_seq=[1, 2, 3, 6, 12, 24, 48, 96, 192],\n",
    "    hidden_size=80,     # RNN cell size\n",
    "    num_layers=3,       # Depth of the network\n",
    "    dropout_rate=0.1,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-8,\n",
    "    batch_size=64,\n",
    "    num_batches_per_epoch=5,\n",
    "    trainer_kwargs={\n",
    "        \"max_epochs\": 10,\n",
    "        \"accelerator\": \"gpu\",\n",
    "        \"gradient_clip_val\": 10.0,\n",
    "        \"logger\": False\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a1313048-1435-41be-adab-6da756bf6580",
   "metadata": {},
   "outputs": [
    {
     "ename": "MisconfigurationException",
     "evalue": "Trainer was configured with `enable_checkpointing=False` but found `ModelCheckpoint` in callbacks list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\86166\\miniforge3\\envs\\563\\Lib\\site-packages\\gluonts\\torch\\model\\estimator.py:237\u001b[0m, in \u001b[0;36mPyTorchLightningEstimator.train\u001b[1;34m(self, training_data, validation_data, shuffle_buffer_length, cache_data, ckpt_path, **kwargs)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain\u001b[39m(\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    230\u001b[0m     training_data: Dataset,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PyTorchPredictor:\n\u001b[1;32m--> 237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle_buffer_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle_buffer_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpredictor\n",
      "File \u001b[1;32mc:\\Users\\86166\\miniforge3\\envs\\563\\Lib\\site-packages\\gluonts\\torch\\model\\estimator.py:203\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(self, training_data, validation_data, from_predictor, shuffle_buffer_length, cache_data, ckpt_path, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\86166\\miniforge3\\envs\\563\\Lib\\site-packages\\pytorch_lightning\\utilities\\argparse.py:70\u001b[0m, in \u001b[0;36minsert_env_defaults\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\86166\\miniforge3\\envs\\563\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:434\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, accelerator, strategy, devices, num_nodes, precision, logger, callbacks, fast_dev_run, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, overfit_batches, val_check_interval, check_val_every_n_epoch, num_sanity_val_steps, log_every_n_steps, enable_checkpointing, enable_progress_bar, enable_model_summary, accumulate_grad_batches, gradient_clip_val, gradient_clip_algorithm, deterministic, benchmark, inference_mode, use_distributed_sampler, profiler, detect_anomaly, barebones, plugins, sync_batchnorm, reload_dataloaders_every_n_epochs, default_root_dir, model_registry)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\86166\\miniforge3\\envs\\563\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:69\u001b[0m, in \u001b[0;36mon_trainer_init\u001b[1;34m(self, callbacks, enable_checkpointing, enable_progress_bar, default_root_dir, enable_model_summary, max_time)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\86166\\miniforge3\\envs\\563\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:91\u001b[0m, in \u001b[0;36m_configure_checkpoint_callbacks\u001b[1;34m(self, enable_checkpointing)\u001b[0m\n",
      "\u001b[1;31mMisconfigurationException\u001b[0m: Trainer was configured with `enable_checkpointing=False` but found `ModelCheckpoint` in callbacks list."
     ]
    }
   ],
   "source": [
    "predictor = estimator.train(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256aee04",
   "metadata": {},
   "source": [
    "Let's modify the DeepAREstimator configuration to fix the training issues. Here's the corrected version:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4fe1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.torch.model.deepar import DeepAREstimator\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# Create a checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='checkpoints',\n",
    "    filename='{epoch}-{train_loss:.2f}',\n",
    "    save_top_k=3,\n",
    "    monitor='train_loss',\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# Configure the estimator\n",
    "estimator = DeepAREstimator(\n",
    "    prediction_length=288,\n",
    "    context_length=288,\n",
    "    freq=\"5min\",\n",
    "    lags_seq=[1, 2, 3, 6, 12, 24, 48, 96, 192],\n",
    "    hidden_size=80,\n",
    "    num_layers=3,\n",
    "    dropout_rate=0.1,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-8,\n",
    "    batch_size=64,\n",
    "    num_batches_per_epoch=5,\n",
    "    trainer_kwargs={\n",
    "        \"max_epochs\": 10,\n",
    "        \"accelerator\": \"gpu\",\n",
    "        \"devices\": 1,\n",
    "        \"gradient_clip_val\": 10.0,\n",
    "        \"logger\": False,\n",
    "        \"callbacks\": [checkpoint_callback],\n",
    "        \"enable_progress_bar\": True,\n",
    "        \"deterministic\": True,\n",
    "        \"auto_select_gpus\": True\n",
    "    }\n",
    ")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Train the model\n",
    "try:\n",
    "    predictor = estimator.train(dataset)\n",
    "except Exception as e:\n",
    "    print(f\"Training error: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb096e41",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Key changes made:\n",
    "1. Added proper checkpoint callback configuration\n",
    "2. Added seed setting for reproducibility\n",
    "3. Added error handling for training\n",
    "4. Specified explicit GPU device\n",
    "5. Added deterministic training\n",
    "6. Removed conflicting checkpoint settings\n",
    "7. Added auto GPU selection\n",
    "\n",
    "The error handling will help identify any issues during training. Make sure you have enough GPU memory available and that your CUDA installation is correct before running this code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a24483",
   "metadata": {},
   "source": [
    "Let's modify the DeepAREstimator configuration to fix the training issues. Here's the corrected version:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94370f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.torch.model.deepar import DeepAREstimator\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# Create a checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='checkpoints',\n",
    "    filename='{epoch}-{train_loss:.2f}',\n",
    "    save_top_k=3,\n",
    "    monitor='train_loss',\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# Configure the estimator\n",
    "estimator = DeepAREstimator(\n",
    "    prediction_length=288,\n",
    "    context_length=288,\n",
    "    freq=\"5min\",\n",
    "    lags_seq=[1, 2, 3, 6, 12, 24, 48, 96, 192],\n",
    "    hidden_size=80,\n",
    "    num_layers=3,\n",
    "    dropout_rate=0.1,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-8,\n",
    "    batch_size=64,\n",
    "    num_batches_per_epoch=5,\n",
    "    trainer_kwargs={\n",
    "        \"max_epochs\": 10,\n",
    "        \"accelerator\": \"gpu\",\n",
    "        \"devices\": 1,\n",
    "        \"gradient_clip_val\": 10.0,\n",
    "        \"logger\": False,\n",
    "        \"callbacks\": [checkpoint_callback],\n",
    "        \"enable_progress_bar\": True,\n",
    "        \"deterministic\": True,\n",
    "        \"auto_select_gpus\": True\n",
    "    }\n",
    ")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Train the model\n",
    "try:\n",
    "    predictor = estimator.train(dataset)\n",
    "except Exception as e:\n",
    "    print(f\"Training error: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1c3b0c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Key changes made:\n",
    "1. Added proper checkpoint callback configuration\n",
    "2. Added seed setting for reproducibility\n",
    "3. Added error handling for training\n",
    "4. Specified explicit GPU device\n",
    "5. Added deterministic training\n",
    "6. Removed conflicting checkpoint settings\n",
    "7. Added auto GPU selection\n",
    "\n",
    "The error handling will help identify any issues during training. Make sure you have enough GPU memory available and that your CUDA installation is correct before running this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f63bc252-6629-4563-a18d-a46fc2299004",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[0;32m      2\u001b[0m forecast_it, ts_it \u001b[38;5;241m=\u001b[39m make_evaluation_predictions(\n\u001b[0;32m      3\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[1;32m----> 4\u001b[0m     predictor\u001b[38;5;241m=\u001b[39m\u001b[43mpredictor\u001b[49m,\n\u001b[0;32m      5\u001b[0m     num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m forecast_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(forecast_it)\n\u001b[0;32m      8\u001b[0m ts_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ts_it)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predictor' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "forecast_it, ts_it = make_evaluation_predictions(\n",
    "    dataset=dataset,\n",
    "    predictor=predictor,\n",
    "    num_samples=100\n",
    ")\n",
    "forecast_list = list(forecast_it)\n",
    "ts_list = list(ts_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1b69e6-5878-43b4-9567-d20cc5f34b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running evaluation: 5it [00:00, 18.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregate Metrics:\n",
      "                 MSE: 1.4311\n",
      "           abs_error: 1045.1602\n",
      "      abs_target_sum: 2389.0000\n",
      "     abs_target_mean: 1.6590\n",
      "      seasonal_error: 0.8808\n",
      "                MASE: 0.9508\n",
      "                MAPE: 0.3061\n",
      "               sMAPE: 0.3974\n",
      "                MSIS: 12.4072\n",
      "num_masked_target_values: 0.0000\n",
      "   QuantileLoss[0.1]: 248.4927\n",
      "       Coverage[0.1]: 0.0035\n",
      "   QuantileLoss[0.2]: 469.2538\n",
      "       Coverage[0.2]: 0.1472\n",
      "   QuantileLoss[0.3]: 696.2949\n",
      "       Coverage[0.3]: 0.2444\n",
      "   QuantileLoss[0.4]: 901.0123\n",
      "       Coverage[0.4]: 0.3778\n",
      "   QuantileLoss[0.5]: 1045.1601\n",
      "       Coverage[0.5]: 0.4049\n",
      "   QuantileLoss[0.6]: 1161.5135\n",
      "       Coverage[0.6]: 0.4333\n",
      "   QuantileLoss[0.7]: 1225.4822\n",
      "       Coverage[0.7]: 0.4590\n",
      "   QuantileLoss[0.8]: 1221.7752\n",
      "       Coverage[0.8]: 0.5292\n",
      "   QuantileLoss[0.9]: 1108.9159\n",
      "       Coverage[0.9]: 0.6611\n",
      "                RMSE: 1.1963\n",
      "               NRMSE: 0.7211\n",
      "                  ND: 0.4375\n",
      "  wQuantileLoss[0.1]: 0.1040\n",
      "  wQuantileLoss[0.2]: 0.1964\n",
      "  wQuantileLoss[0.3]: 0.2915\n",
      "  wQuantileLoss[0.4]: 0.3772\n",
      "  wQuantileLoss[0.5]: 0.4375\n",
      "  wQuantileLoss[0.6]: 0.4862\n",
      "  wQuantileLoss[0.7]: 0.5130\n",
      "  wQuantileLoss[0.8]: 0.5114\n",
      "  wQuantileLoss[0.9]: 0.4642\n",
      "mean_absolute_QuantileLoss: 897.5445\n",
      "  mean_wQuantileLoss: 0.3757\n",
      "        MAE_Coverage: 0.2537\n",
      "                 OWA: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuximin/miniforge3/envs/satcast/lib/python3.12/site-packages/gluonts/json.py:102: UserWarning: Using `json`-module for json-handling. Consider installing one of `orjson`, `ujson` to speed up serialization and deserialization.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluation\n",
    "evaluator = Evaluator()\n",
    "agg_metrics, item_metrics = evaluator(ts_list, forecast_list)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nAggregate Metrics:\")\n",
    "for k, v in agg_metrics.items():\n",
    "    print(f\"{k:>20}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ea325f-626f-447a-85f1-2b1bbd3b3e37",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'forecast_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m n_plots \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mforecast_list\u001b[49m)  \u001b[38;5;66;03m# or set a lower limit for visual clarity\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_plots):\n\u001b[0;32m      3\u001b[0m     ts \u001b[38;5;241m=\u001b[39m ts_list[i]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'forecast_list' is not defined"
     ]
    }
   ],
   "source": [
    "n_plots = len(forecast_list)  # or set a lower limit for visual clarity\n",
    "for i in range(n_plots):\n",
    "    ts = ts_list[i]\n",
    "    forecast = forecast_list[i]\n",
    "\n",
    "    # Convert PeriodIndex to Timestamp for plotting\n",
    "    ts_index = ts.index.to_timestamp()\n",
    "\n",
    "    # Forecast timestamps\n",
    "    start = forecast.start_date.to_timestamp()\n",
    "    freq = pd.Timedelta(forecast.freq)\n",
    "    forecast_index = pd.date_range(start=start, periods=len(forecast.mean), freq=freq)\n",
    "\n",
    "    # Determine x-axis limits (last 2 days)\n",
    "    last_time = ts_index[-1]\n",
    "    xlim_start = last_time - pd.Timedelta(days=2)\n",
    "    xlim_end = last_time + pd.Timedelta(minutes=5 * len(forecast.mean))  # include forecast horizon\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.plot(ts_index, ts.values, label=\"True values\", color=\"black\")\n",
    "    plt.plot(forecast_index, forecast.mean, label=\"Forecast (mean)\", color=\"blue\")\n",
    "\n",
    "    plt.title(f\"Forecast vs True (Series {i})\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xlim(xlim_start, xlim_end)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea6114",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import optuna\n",
    "from gluonts.evaluation import Evaluator\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.model.deepar import DeepAREstimator\n",
    "from gluonts.trainer import Trainer\n",
    "\n",
    "def objective(trial):\n",
    "    hs = trial.suggest_categorical(\"hidden_size\", [40, 80, 120])\n",
    "    cl = trial.suggest_categorical(\"context_length\", [144, 288])\n",
    "    dr = trial.suggest_float(\"dropout_rate\", 0.0, 0.3)\n",
    "\n",
    "    estimator = DeepAREstimator(\n",
    "        prediction_length=prediction_length,\n",
    "        context_length=cl,\n",
    "        freq=freq,\n",
    "        hidden_size=hs,\n",
    "        dropout_rate=dr,\n",
    "        trainer=Trainer(epochs=5)\n",
    "    )\n",
    "\n",
    "    predictor = estimator.train(train_ds)\n",
    "\n",
    "    forecast_it, ts_it = make_evaluation_predictions(dataset=test_ds, predictor=predictor, num_samples=100)\n",
    "    agg_metrics, _ = Evaluator()(ts_it, forecast_it)\n",
    "\n",
    "    return agg_metrics[\"RMSE\"]\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "study.best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d17b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for forecast in forecasts:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    forecast.plot(color='g')\n",
    "    plt.plot(prediction_index, target[-prediction_length:], color='orange')\n",
    "    plt.fill_between(prediction_index, forecast.quantile(0.1), forecast.quantile(0.9), color='lightblue', alpha=0.4)\n",
    "    plt.title(\"Forecast with 80% Prediction Interval\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b89005",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "external_columns = ['mempool_blocks_nTx']\n",
    "exog_values = df[external_columns].T.values.tolist()\n",
    "\n",
    "train_ds = ListDataset([{\n",
    "    \"target\": df[target_column][:train_end_index].values,\n",
    "    \"start\": df.index[0],\n",
    "    \"feat_dynamic_real\": exog_values\n",
    "}], freq=freq)\n",
    "\n",
    "test_ds = ListDataset([{\n",
    "    \"target\": df[target_column].values,\n",
    "    \"start\": df.index[0],\n",
    "    \"feat_dynamic_real\": exog_values\n",
    "}], freq=freq)\n",
    "\n",
    "estimator = DeepAREstimator(\n",
    "    prediction_length=prediction_length,\n",
    "    context_length=288,\n",
    "    freq=freq,\n",
    "    use_feat_dynamic_real=True,\n",
    "    trainer=Trainer(epochs=5)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d355392",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "horizons = [72, 144, 288]\n",
    "results = []\n",
    "\n",
    "for horizon in horizons:\n",
    "    estimator = DeepAREstimator(\n",
    "        prediction_length=horizon,\n",
    "        context_length=horizon * 2,\n",
    "        freq=freq,\n",
    "        trainer=Trainer(epochs=5)\n",
    "    )\n",
    "    predictor = estimator.train(train_ds)\n",
    "    forecast_it, ts_it = make_evaluation_predictions(test_ds, predictor=predictor, num_samples=100)\n",
    "    agg_metrics, _ = Evaluator()(ts_it, forecast_it)\n",
    "    results.append((horizon, agg_metrics[\"RMSE\"], agg_metrics[\"MAE\"]))\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=[\"Horizon\", \"RMSE\", \"MAE\"])\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a09c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "split_points = list(range(5000, len(df), 288))\n",
    "metrics_expanding = []\n",
    "\n",
    "for split in split_points:\n",
    "    train_target = df[target_column][:split]\n",
    "    test_target = df[target_column][split:split+288]\n",
    "\n",
    "    train_ds = ListDataset([{\n",
    "        \"target\": train_target.values,\n",
    "        \"start\": df.index[0],\n",
    "        \"feat_dynamic_real\": [df[\"mempool_blocks_nTx\"].values]\n",
    "    }], freq=freq)\n",
    "\n",
    "    test_ds = ListDataset([{\n",
    "        \"target\": df[target_column].values,\n",
    "        \"start\": df.index[0],\n",
    "        \"feat_dynamic_real\": [df[\"mempool_blocks_nTx\"].values]\n",
    "    }], freq=freq)\n",
    "\n",
    "    estimator = DeepAREstimator(\n",
    "        prediction_length=288,\n",
    "        context_length=288,\n",
    "        freq=freq,\n",
    "        use_feat_dynamic_real=True,\n",
    "        trainer=Trainer(epochs=5)\n",
    "    )\n",
    "    predictor = estimator.train(train_ds)\n",
    "    forecast_it, ts_it = make_evaluation_predictions(test_ds, predictor=predictor, num_samples=100)\n",
    "    agg_metrics, _ = Evaluator()(ts_it, forecast_it)\n",
    "    metrics_expanding.append((df.index[split], agg_metrics[\"RMSE\"], agg_metrics[\"MAE\"]))\n",
    "\n",
    "metrics_expanding_df = pd.DataFrame(metrics_expanding, columns=[\"SplitTime\", \"RMSE\", \"MAE\"])\n",
    "metrics_expanding_df.plot(x=\"SplitTime\", y=[\"RMSE\", \"MAE\"], title=\"Expanding Window Evaluation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685185d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b079e4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59ee18f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2c9630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1839916c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87799594",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
